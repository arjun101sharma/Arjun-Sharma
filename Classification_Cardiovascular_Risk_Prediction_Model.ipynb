{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yiiVWRdJDDil"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjun101sharma/Arjun-Sharma/blob/main/Classification_Cardiovascular_Risk_Prediction_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**  -  Cardiovascular Risk Prediction."
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Project Type**    - Classification\n",
        "#### **Contribution**    - Individual\n",
        "#### **Team Member 1**-  $\\color{orange}{\\text{Arjun Sharma}}$"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing** :\n",
        "\n",
        "1. Getting the dataset\n",
        "2. Importing libraries\n",
        "3. Importing datasets\n",
        "4. Finding Missing Data\n",
        "5. Encoding Categorical Data\n",
        "6. Data Cleaning and Feature Engineering\n",
        "\n",
        "**Exploratory data analysis(EDA) :**\n",
        "\n",
        "1. Count of Presence or Absence of Cardiovascular Disease.\n",
        "\n",
        "2. The number of individuals with/without cardiovascular disease. For both males and females??\n",
        "\n",
        "3. Distribution of number of people with/without cardiovascular disease of Males and Females for the different age groups in count plot??\n",
        "\n",
        "4. Distribution of Heart rate measure of Males and Females for the different age groups in line plot??\n",
        "\n",
        "5. Distribution of number of people with/without cardiovascular disease of Males and Females for the different age groups in line plot??\n",
        "\n",
        "6. Age Distribution of people having Cardiovascular disease and not having Cardiovascular disease in violin plot??\n",
        "\n",
        "7. Distribution of Cholesterol measure of Males and Females for the different age groups in line plot??\n",
        "\n",
        "8. Bar plot for cardiac risk disease for different cholesterol level.\n",
        "\n",
        "9. Bar plot for cardiac risk disease based on daily cigarettes consumption.\n",
        "\n",
        "10. Correlation Heatmap.\n",
        "\n",
        "11. Pair Plot.\n",
        "\n",
        "**Supervised Classification Machine learning algorithms and implementation :**\n",
        "\n",
        "1. Logistic regression.\n",
        "\n",
        "2. Decision Tree.\n",
        "\n",
        "3. Random Forest.\n",
        "\n",
        "4. SVM (Support Vector Machine).\n",
        "\n",
        "5. Xtreme Gradient Boosting.\n",
        "\n",
        "6. Naive Bayes.\n",
        "\n",
        "7. Neural Network.\n",
        "\n",
        "8. Selection of best model."
      ],
      "metadata": {
        "id": "dv0Vn0J5pAx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cardiovascular diseases (CVDs) are a leading cause of mortality and morbidity worldwide. Early prediction and identification of individuals at high risk of developing CVDs are crucial for effective preventive strategies and personalized healthcare interventions. This project aims to develop and evaluate a robust cardiovascular risk prediction model using advanced machine learning techniques.\n",
        "\n",
        "Objectives:\n",
        "The primary objective of this project is to create an accurate and reliable machine learning model capable of predicting an individual's risk of developing cardiovascular diseases within a specified time frame. Secondary objectives include identifying key risk factors, assessing the model's performance, and comparing it with existing risk assessment tools.\n",
        "\n",
        "Methodology:\n",
        "\n",
        "Data Collection: A diverse and comprehensive dataset containing demographic information, medical history, lifestyle factors, and clinical measurements will be collected from electronic health records, surveys, and relevant literature.\n",
        "\n",
        "Feature Engineering: Relevant features will be selected and engineered from the dataset. Feature scaling, normalization, and handling missing values will be performed to ensure data quality.\n",
        "\n",
        "Model Development: Various machine learning algorithms, including but not limited to logistic regression, random forests, support vector machines, and gradient boosting, will be employed to develop the risk prediction model. Hyperparameter tuning will be conducted to optimize model performance.\n",
        "\n",
        "Validation and Evaluation: The model will be validated using cross-validation techniques and evaluated using metrics such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC).\n",
        "\n",
        "Comparison with Existing Models: The developed model's performance will be compared with established cardiovascular risk assessment tools, such as the Framingham Risk Score and the SCORE risk charts, to determine its superiority in accuracy and reliability.\n",
        "\n",
        "Interpretability and Visualization: Model interpretability techniques, such as feature importance analysis and SHAP (SHapley Additive exPlanations) values, will be employed to gain insights into the factors driving the predictions. Visualization tools will be used to communicate these findings effectively.\n",
        "\n",
        "Results and Implications:\n",
        "The successful development of a robust cardiovascular risk prediction model holds several potential implications for healthcare and clinical practice:\n",
        "\n",
        "Personalized Risk Assessment: The model can provide individuals with a personalized risk score, enabling them to take proactive steps to mitigate their risk factors and adopt healthier lifestyles.\n",
        "\n",
        "Resource Allocation: Healthcare systems can allocate resources more efficiently by identifying individuals with high predicted risk, allowing targeted interventions and reducing the overall burden of CVDs.\n",
        "\n",
        "Preventive Strategies: The model's identification of key risk factors can guide the development of more effective preventive strategies, including public health campaigns and early interventions.\n",
        "\n",
        "Clinical Decision Support: Clinicians can utilize the model's predictions to make informed decisions about patient care, focusing on high-risk individuals who may benefit from closer monitoring and more aggressive interventions.\n",
        "\n",
        "Conclusion:\n",
        "The development of an accurate and reliable cardiovascular risk prediction model using advanced machine learning techniques has the potential to revolutionize the way we approach cardiovascular disease prevention and management. By leveraging comprehensive datasets and state-of-the-art algorithms, this project aims to provide a valuable tool for personalized healthcare and public health initiatives, ultimately contributing to the reduction of cardiovascular disease burden in the population."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/arjun101sharma/Arjun-Sharma"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cardiovascular diseases (CVDs) are a leading cause of mortality and morbidity worldwide. Early identification and accurate prediction of cardiovascular risk factors are crucial for effective prevention and intervention strategies. Traditional risk assessment models often rely on limited clinical parameters and may not provide personalized predictions.\n",
        "\n",
        "The challenge lies in developing a robust and accurate cardiovascular risk prediction model that integrates a wide range of patient-specific data, including demographic information, medical history, lifestyle factors, and biomarker measurements. This model should surpass the limitations of existing approaches by leveraging advanced machine learning techniques to handle complex interactions between various risk factors and improve prediction accuracy.\n",
        "\n",
        "Key Objectives:\n",
        "\n",
        "Data Integration: Gather and preprocess diverse datasets encompassing patient demographics, medical records, genetic information, lifestyle habits, and relevant biomarker measurements.\n",
        "\n",
        "Feature Engineering: Identify informative features and develop novel feature engineering strategies that capture intricate relationships between risk factors, potentially involving nonlinear interactions and higher-order correlations.\n",
        "\n",
        "Model Development: Explore and implement state-of-the-art machine learning algorithms, such as deep learning, ensemble methods, and feature selection techniques, to construct a comprehensive risk prediction model.\n",
        "\n",
        "Personalization: Create a model that generates personalized risk assessments, accounting for individual variations in genetics, lifestyle, and medical history. This should move beyond one-size-fits-all approaches.\n",
        "\n",
        "Interpretability: Strive to achieve transparency and interpretability in the model's predictions, enabling medical professionals to understand the rationale behind risk assessments and facilitating patient-doctor communication.\n",
        "\n",
        "Evaluation: Rigorously evaluate the developed model using appropriate performance metrics and benchmark it against existing risk prediction models. Cross-validation and external validation should be employed to assess the model's generalization ability.\n",
        "\n",
        "Clinical Applicability: Ensure the model's feasibility for integration into clinical workflows. Develop user-friendly interfaces that allow healthcare providers to input patient data easily and obtain risk predictions promptly.\n",
        "\n",
        "Ethical Considerations: Address potential biases in the data and model predictions, and implement measures to mitigate these biases. Ensure patient data privacy and compliance with relevant regulations (e.g., GDPR, HIPAA).\n",
        "\n",
        "The successful completion of this project will yield a cutting-edge cardiovascular risk prediction model that contributes to more accurate and personalized preventive healthcare strategies. By providing clinicians and patients with actionable insights, this model has the potential to reduce the burden of cardiovascular diseases and improve overall public health."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know our Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import necessary libraries for data analysis and visualization\n",
        "# Data manipulation and analysis\n",
        "# Importing pandas library for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Importing numpy library for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Importing matplotlib for basic plotting and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Importing rcParams for customizing plot parameters\n",
        "from matplotlib import rcParams\n",
        "\n",
        "# Importing SMOTE and SMOTETomek from imblearn for oversampling techniques\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# Importing rainbow colormap for visualizations\n",
        "from matplotlib.cm import rainbow\n",
        "\n",
        "# Magic command to display plots inline in Jupyter notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "# Importing warnings to manage and suppress warnings\n",
        "import warnings\n",
        "\n",
        "# Suppressing warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importing plotly express for statistical data visualization\n",
        "import plotly.express as px\n",
        "\n",
        "# Importing seaborn for statistical data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing xgboost for gradient boosting algorithm\n",
        "import xgboost as xgb\n",
        "\n",
        "# Importing mathematical functions from numpy\n",
        "from numpy import math\n",
        "\n",
        "# Importing train_test_split from sklearn for splitting data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Importing StandardScaler from sklearn for feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Importing various metrics from sklearn for model evaluation\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "# Importing RepeatedStratifiedKFold for cross-validation\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "# Importing specific metrics from sklearn for evaluation\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "\n",
        "# Importing GridSearchCV and RandomizedSearchCV from sklearn for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Importing classifiers from sklearn for modeling\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Importing XGBClassifier from xgboost for extreme gradient boosting\n",
        "from xgboost import XGBClassifier as xgb\n",
        "\n",
        "# Additional metrics imports for model evaluation\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "sJgfPLdCIZam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code snippet demonstrates the use of the chardet library to detect the character encoding of a given CSV file.\n",
        "file = \"/content/data_cardiovascular_risk.csv\"\n",
        "import chardet\n",
        "\n",
        "# The file path is specified, and the chardet library is used to analyze the first 100,000 bytes of the file's raw binary data.\n",
        "with open(file, 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(100000))\n",
        "\n",
        "# The detected character encoding information is then stored in the 'result' variable, which can be used to determine the appropriate encoding for reading the file.\n",
        "result"
      ],
      "metadata": {
        "id": "lLtkO5dOEF-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "hd_df = pd.read_csv(\"/content/data_cardiovascular_risk.csv\")"
      ],
      "metadata": {
        "id": "SZk2GporHv0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Dataset head Look\n",
        "hd_df.head() # Display the first 5 rows of the DataFrame.\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset tail Look\n",
        "hd_df.tail()  # Display the last 5 rows of the DataFrame."
      ],
      "metadata": {
        "id": "h3wAlNhXIIKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of rows in the dataset using the 'shape' attribute\n",
        "print(f'Number of rows in the data set is {hd_df.shape[0]}')\n",
        "\n",
        "# Display the number of columns in the dataset using the 'shape' attribute\n",
        "print(f'Number of Columns in the data set is {hd_df.shape[1]}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "hd_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_counts = hd_df.duplicated(keep=False).sum()\n",
        "\n",
        "# Display the count of duplicate rows\n",
        "print(\"Number of duplicate rows:\", duplicate_counts)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Count missing values in each column\n",
        "missing_values_count = hd_df.isnull().sum()\n",
        "print(missing_values_count)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Setting the figure size for the visualization\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Creating a heatmap to visualize missing values in the DataFrame\n",
        "sns.heatmap(hd_df.isnull(), cmap='plasma', annot=False, yticklabels=False)\n",
        "\n",
        "# Adding a title to the visualization\n",
        "plt.title(\"Visualizing Missing Values\")\n",
        "\n",
        "# Displaying the visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vittnDd_NXtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here.**\n",
        "#### The data set has 3390 rows 18 columns.\n",
        "#### Data types our data set are: float64(9), int64(6), object(2).\n",
        "#### Zero Duplicate values/rows.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "hd_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "hd_df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "#### **Age:**\n",
        "The age of the individual is a significant factor in cardiovascular risk assessment, as the risk of CVD tends to increase with age.\n",
        "\n",
        "#### **Previous Cardiovascular Events:**\n",
        "A history of heart attack, stroke, or other cardiovascular events increases the risk of future events.\n",
        "#### **Diabetes:**\n",
        "Individuals with diabetes are at higher risk of CVD.\n",
        "#### **Hypertension:**\n",
        "High blood pressure is a major risk factor for CVD.\n",
        "#### **Hyperlipidemia:**\n",
        "Abnormal lipid levels can contribute to CVD risk.\n",
        "\n",
        "#### **Cholesterol Levels:**\n",
        "Levels of total cholesterol, HDL cholesterol (good cholesterol), LDL cholesterol (bad cholesterol), and triglycerides are important predictors of CVD risk.\n",
        "#### **Body Mass Index (BMI):**\n",
        "BMI is used as an indicator of obesity, which is associated with increased CVD risk.\n",
        "#### **Smoking Status:**\n",
        " Whether the individual is a smoker or not is a significant risk factor for CVD.\n",
        "\n",
        "\n",
        "#### **Demographic Information:**\n",
        "\n",
        "#### **Gender:**\n",
        "The patient's sex, indicated as either \"M\" for male or \"F\" for female.\n",
        "#### **Age:**\n",
        " The patient's age, represented as a continuous variable. Although the recorded ages are rounded to whole numbers, age is actually a continuous concept.\n",
        "#### **Education:**\n",
        "The patient's level of education categorized as values 1, 2, 3, or 4.\n",
        "\n",
        "#### **Behavioral Information:**\n",
        "\n",
        "#### **Smoking Status:**\n",
        "Indicates whether the patient is a current smoker, with values \"YES\" for yes and \"NO\" for no.\n",
        "#### **Cigarettes Per Day:**\n",
        "The average number of cigarettes smoked by the individual per day. This can be considered continuous, as any number of cigarettes, even fractions like half a cigarette, can be used.\n",
        "\n",
        "#### **Medical History:**\n",
        "\n",
        "#### **Blood Pressure Medication:**\n",
        "Indicates whether the patient was taking blood pressure medication, categorized as nominal (no specific numerical values).\n",
        "#### **Previous Stroke:**\n",
        "Indicates whether the patient had previously experienced a stroke, categorized as nominal (no specific numerical values).\n",
        "Hypertension Status: Indicates whether the patient had hypertension (high blood pressure), categorized as nominal (no specific numerical values).\n",
        "Diabetes Status: Indicates whether the patient had diabetes, categorized as nominal (no specific numerical values).\n",
        "\n",
        "#### **Current Medical Information:**\n",
        "\n",
        "#### **Total Cholesterol:**\n",
        "The patient's total cholesterol level, represented as a continuous variable.\n",
        "#### **Systolic Blood Pressure:**\n",
        "The patient's systolic blood pressure, represented as a continuous variable.\n",
        "#### **Diastolic Blood Pressure:**\n",
        "The patient's diastolic blood pressure, represented as a continuous variable.\n",
        "#### **Body Mass Index (BMI):**\n",
        "The patient's Body Mass Index, represented as a continuous variable.\n",
        "#### **Heart Rate:**\n",
        "The patient's heart rate, treated as a continuous variable. Although heart rate is discrete in reality, it is considered continuous in medical research due to its large range of possible values.\n",
        "#### **Glucose Level:**\n",
        "The patient's glucose level, represented as a continuous variable.\n",
        "\n",
        "#### **Predictive Variable (Target):**\n",
        "\n",
        "10-Year Risk of Coronary Heart Disease (CHD): A binary variable where \"1\" indicates a \"Yes\" for a 10-year risk of coronary heart disease, and \"0\" indicates a \"No.\""
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable.\n"
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Display unique values for each variable\n",
        "for column in hd_df.columns:\n",
        "    unique_values = hd_df[column].nunique()\n",
        "    print(f\"Number of Unique values for {column}:\\n{unique_values}.\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Renaming columns in the hd_df DataFrame for improved readability and clarity.\n",
        "# The new column names are chosen to provide more intuitive names and consistency.\n",
        "hd_df.rename(columns={'sex':'gender','age':'Age','is_smoking':'Weather_smoking_or_not','cigsPerDay':'Cigrets_smoked_per_day'\n",
        ",'BPMeds':'Weather_taking_BP_meds_or_not','prevalentStroke':'If_the_patient_has_a_history_of_stroke'\n",
        "                     ,'prevalentHyp':'If_the_patient_has_a_history_of_hypertension',\n",
        "                      'diabetes':'Patient_has_diabeties_or_not','totChol':'Cholesterol_measure','sysBP':'Systolic_BP_measure'\n",
        "                    ,'diaBP':'Diastolic_BP_measure','BMI':'Body_Mass_Index'\n",
        "                    ,'heartRate':'Heart_Rate_measure','TenYearCHD':'Presence_or_absence_of_cardiovascular_disease'},\n",
        "                inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'gender' column values from 'M' and 'F' to 1 and 0 respectively\n",
        "hd_df['gender'] = hd_df['gender'].replace({'M': 1, 'F': 0})\n",
        "\n",
        "# Convert 'Weather_smoking_or_not' column values from 'YES' and 'NO' to 1 and 0 respectively\n",
        "hd_df['Weather_smoking_or_not'] = hd_df['Weather_smoking_or_not'].replace({'YES': 1, 'NO': 0})\n",
        "\n",
        "# Fill missing values in specific columns with their median values\n",
        "hd_df['Cigrets_smoked_per_day'] = hd_df['Cigrets_smoked_per_day'].fillna(hd_df['Cigrets_smoked_per_day'].median())\n",
        "hd_df['Body_Mass_Index'] = hd_df['Body_Mass_Index'].fillna(hd_df['Body_Mass_Index'].median())\n",
        "hd_df['Heart_Rate_measure'] = hd_df['Heart_Rate_measure'].fillna(hd_df['Heart_Rate_measure'].median())\n",
        "hd_df['Cholesterol_measure'] = hd_df['Cholesterol_measure'].fillna(hd_df['Cholesterol_measure'].median())\n",
        "hd_df['glucose'] = hd_df['glucose'].fillna(hd_df['glucose'].median())\n",
        "hd_df['education'] = hd_df['education'].fillna(hd_df['education'].median())\n",
        "hd_df['Weather_taking_BP_meds_or_not'] = hd_df['Weather_taking_BP_meds_or_not'].fillna(hd_df['Weather_taking_BP_meds_or_not'].median())\n",
        "\n",
        "# Convert all columns to integer data type\n",
        "for var in hd_df.columns:\n",
        "    hd_df[var] = hd_df[var].astype(int)\n",
        "\n",
        "# Create a copy of the dataset and perform one-hot encoding on the 'education' column\n",
        "data_set = hd_df.copy()\n",
        "data_set = pd.get_dummies(data_set, columns=['education'])\n",
        "\n"
      ],
      "metadata": {
        "id": "az0ktR5ftE9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating list of numerical and categorical columns\n",
        "continuous_variable=[]\n",
        "for col in hd_df.columns:\n",
        "  if hd_df[col].nunique()>4:\n",
        "    continuous_variable.append(col)"
      ],
      "metadata": {
        "id": "n6aKaJxst6Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(continuous_variable)\n",
        "continuous_variable.remove('id')\n",
        "print(continuous_variable)"
      ],
      "metadata": {
        "id": "c4L3ntL0t6EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "#Checking for outliers\n",
        "\n",
        "fig = plt.figure(figsize=(8,25))\n",
        "c=1\n",
        "for i in continuous_variable :\n",
        "    plt.subplot(10, 4, c)\n",
        "    plt.xlabel('Distibution of {}'.format(i))\n",
        "    sns.boxplot(x=i,data=hd_df,color=\"tomato\")\n",
        "    c = c + 1\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
      ],
      "metadata": {
        "id": "n0XikF4Bt59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for outliers using normal distribution plots\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Number of columns and rows for subplots\n",
        "num_cols = 4\n",
        "num_rows = len(continuous_variable) // num_cols + 1\n",
        "\n",
        "# Iterate through continuous variables and create subplots\n",
        "for index, variable in enumerate(continuous_variable, start=1):\n",
        "    plt.subplot(num_rows, num_cols, index)\n",
        "\n",
        "    # Plot the normal distribution using histogram with kernel density estimate (KDE)\n",
        "    sns.histplot(x=variable, data=hd_df, kde=True, color=\"skyblue\", stat=\"density\")\n",
        "\n",
        "    # Set plot title and labels\n",
        "    #plt.title(f'{variable}', fontsize=14)\n",
        "    plt.xlabel(variable, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "\n",
        "    # Add mean and median lines for reference\n",
        "    plt.axvline(hd_df[variable].mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\n",
        "    plt.axvline(hd_df[variable].median(), color='green', linestyle='dashed', linewidth=1, label='Median')\n",
        "\n",
        "    # Show legend\n",
        "    plt.legend()\n",
        "\n",
        "# Adjust layout to prevent overlapping of subplots\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2HrnkBYuMAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Number of columns and rows for subplots\n",
        "num_cols = 2\n",
        "num_rows = len(continuous_variable)\n",
        "\n",
        "# Iterate through continuous variables and create subplots\n",
        "for index, variable in enumerate(continuous_variable, start=1):\n",
        "    # Box Plot\n",
        "    plt.subplot(num_rows, num_cols, index * 2 - 1)\n",
        "    sns.boxplot(x=hd_df[variable], color=\"tomato\")\n",
        "    plt.xlabel('Boxplot of {}'.format(variable))\n",
        "\n",
        "    # Distribution Plot with KDE\n",
        "    plt.subplot(num_rows, num_cols, index * 2)\n",
        "    sns.histplot(x=hd_df[variable], kde=True, color=\"skyblue\", stat=\"density\")\n",
        "\n",
        "    # Set plot title and labels for distribution plot\n",
        "    plt.title(f'Distribution of {variable}', fontsize=14)\n",
        "    plt.xlabel(variable, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "\n",
        "    # Add mean and median lines for reference\n",
        "    plt.axvline(hd_df[variable].mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\n",
        "    plt.axvline(hd_df[variable].median(), color='green', linestyle='dashed', linewidth=1, label='Median')\n",
        "\n",
        "    # Show legend for distribution plot\n",
        "    plt.legend()\n",
        "\n",
        "# Adjust layout to prevent overlapping of subplots\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pxQza0zDPNt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Managing Extreme Values and Dealing with Outliers\n",
        "for column in continuous_variable:\n",
        "  Q_1,Medium,Q_3 = hd_df[column].quantile([0.25,0.5,0.75])\n",
        "  Lower_Limit = Q_1 - 1.5*(Q_3-Q_1)\n",
        "  Upper_Limit = Q_3 + 1.5*(Q_3-Q_1)\n",
        "\n",
        "\n",
        "  # Substituting values that fall far from the typical range.\n",
        "  hd_df[column] = np.where(hd_df[column] > Upper_Limit,Upper_Limit,np.where(\n",
        "                              hd_df[column] < Lower_Limit,Lower_Limit,hd_df[column]))"
      ],
      "metadata": {
        "id": "dQP15ycm-NJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Number of columns and rows for subplots\n",
        "num_cols = 2\n",
        "num_rows = len(continuous_variable)\n",
        "\n",
        "# Iterate through continuous variables and create subplots\n",
        "for index, variable in enumerate(continuous_variable, start=1):\n",
        "    # Box Plot\n",
        "    plt.subplot(num_rows, num_cols, index * 2 - 1)\n",
        "    sns.boxplot(x=hd_df[variable], color=\"tomato\")\n",
        "    plt.xlabel('Boxplot of {}'.format(variable))\n",
        "\n",
        "    # Distribution Plot with KDE\n",
        "    plt.subplot(num_rows, num_cols, index * 2)\n",
        "    sns.histplot(x=hd_df[variable], kde=True, color=\"skyblue\", stat=\"density\")\n",
        "\n",
        "    # Set plot title and labels for distribution plot\n",
        "    plt.title(f'Distribution of {variable}', fontsize=14)\n",
        "    plt.xlabel(variable, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "\n",
        "    # Add mean and median lines for reference\n",
        "    plt.axvline(hd_df[variable].mean(), color='red', linestyle='dashed', linewidth=1, label='Mean')\n",
        "    plt.axvline(hd_df[variable].median(), color='green', linestyle='dashed', linewidth=1, label='Median')\n",
        "\n",
        "    # Show legend for distribution plot\n",
        "    plt.legend()\n",
        "\n",
        "# Adjust layout to prevent overlapping of subplots\n",
        "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJRJJICQn-Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_df.head()"
      ],
      "metadata": {
        "id": "SUsTGqWcSk7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_df.info()"
      ],
      "metadata": {
        "id": "yDMB5A78SYNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Altered the column names to make them more user-friendly and convenient.\n",
        "Convert categorical variables into binary-encoded columns using one-hot encoding."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used drop null values technique(dropna(subset=[column], inplace=True)).I remove all the rows having null values in my dataset null rows are less in numbers It won't reduce my dateset rows much."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 1\n",
        "## Count of Presence or Absence of Cardiovascular Disease.\n"
      ],
      "metadata": {
        "id": "_B1rsuWj86W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code.\n",
        "\n",
        "# Set the plotting style to 'darkgrid' for better aesthetics\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# Create a countplot using seaborn for the target variable with an appropriate color palette\n",
        "ax = sns.countplot(x=hd_df['Presence_or_absence_of_cardiovascular_disease'], palette='summer')\n",
        "\n",
        "# Set the label for the x-axis and adjust the font size and padding\n",
        "plt.xlabel('Presence or Absence of Cardiovascular Disease', labelpad=15, size=14, fontweight='bold')\n",
        "\n",
        "# Set the label for the y-axis and adjust the font size\n",
        "plt.ylabel('Count', size=14, fontweight='bold')\n",
        "\n",
        "# Set the title of the plot and adjust the font size\n",
        "plt.title('Distribution of Cardiovascular Disease',  size=16, fontweight='bold')\n",
        "\n",
        "# Improve visibility of x-axis labels\n",
        "ax.set_xticklabels(['No Cardiovascular Disease', 'Cardiovascular Disease'], fontsize=12)\n",
        "\n",
        "# Set the size of the figure for better visibility\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Annotate each bar with its count to provide detailed information on the plot\n",
        "for p in ax.patches:\n",
        "    # Annotate the bar with count at an appropriate position and adjust font size\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=12, color='black', xytext=(0, 10),\n",
        "                textcoords='offset points')\n",
        "\n",
        "# Show the plot for visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z4KG1aJt3oVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "N_XAKdBQo2qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here.**\n",
        "\n",
        "Countplot charts are commonly employed to illustrate the relative portions of a complete entity, making them particularly effective for representing data that has been computed as a percentage of the whole."
      ],
      "metadata": {
        "id": "8q7IK_wEo2XU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "From the above chart we come to know that 15.1% that is 511 out of 3390 are classified as positive for 10 year CHD whereas the remaining 84.9% that is 2879 out of 3390 are classified as negative for 10 year CHD.\n"
      ],
      "metadata": {
        "id": "GUM2Q6K8o2RM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The insights from the chart show that 15.1% of the population has a positive classification for 10-year CHD, while 84.9% have a negative classification. This information can help businesses in the healthcare industry develop targeted strategies. There are no specific insights in the chart that indicate negative growth, but failure to address high CHD prevalence could have negative implications for public health and healthcare businesses.\n"
      ],
      "metadata": {
        "id": "CJscuGtEo2AW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart-2\n",
        "## The number of individuals with/without cardiovascular disease. For both males and females??\n",
        "\n"
      ],
      "metadata": {
        "id": "qrBElxp_ugS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Set the seaborn style\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Create the countplot with appropriate color palette\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size for better visibility\n",
        "ax = sns.countplot(data=hd_df, x='gender', hue='Presence_or_absence_of_cardiovascular_disease', palette='Set2')\n",
        "\n",
        "# Annotate the bars with counts\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', fontsize=12, color='black')\n",
        "\n",
        "# Set labels and title with appropriate font size and style\n",
        "plt.xlabel('Gender', size=14, fontweight='bold')\n",
        "plt.ylabel('Count', size=14, fontweight='bold')\n",
        "plt.title('Distribution of Cardiovascular Disease by Gender', size=16, fontweight='bold')\n",
        "\n",
        "# Customize legend\n",
        "legend_labels = ['No Cardiovascular Disease', 'Cardiovascular Disease']\n",
        "plt.legend(title='Health Status', labels=legend_labels, title_fontsize='14', fontsize='12')\n",
        "\n",
        "# Improve visibility of x-axis labels\n",
        "ax.set_xticklabels(['Male', 'Female'], fontsize=12)\n",
        "\n",
        "# Improve grid visibility\n",
        "ax.grid(visible=True, linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Add a horizontal line at y=0 for better reference\n",
        "plt.axhline(y=0, color='black', linewidth=0.5, linestyle='--')\n",
        "\n",
        "# Add plot border\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cr9jdPn122gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "We have chosen an appropriate chart type (grouped bar chart) and made several design decisions to ensure clarity and readability, making it an effective way to represent and compare the distribution of cardiovascular disease cases by gender.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The heights of the bars represent the count of individuals in the dataset. For both males and females, we can see how many individuals are present in the dataset.\n",
        "Out of total 1923 Males 1684 Males do not have cardiovascular disease and 239 Males do have cardiovascular disease.\n",
        "Out of total 1467 Females 1195 Females do not have cardiovascular disease and 272 Females do have cardiovascular disease.\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "**Insights:**\n",
        "\n",
        "The chart shows that there are more females with cardiovascular disease than males. This might indicate that cardiovascular disease is more prevalent among females in the given dataset.\n",
        "The number of individuals without cardiovascular disease appears higher for males compared to females.\n",
        "**Potential Positive Business Impact:**\n",
        "\n",
        "**Targeted Marketing and Healthcare Services:** If a business operates in the healthcare industry, especially in areas related to cardiovascular health, this insight can guide targeted marketing efforts and the development of specialized healthcare services. For instance, there could be a demand for specialized cardiovascular clinics for women.\n",
        "\n",
        "**Product Development:** Companies manufacturing products related to cardiovascular health might consider developing products tailored to the needs of females, considering the higher prevalence indicated in the dataset. These products could include specialized medications, health supplements, or medical devices.\n",
        "\n",
        "**Health Insurance and Risk Assessment:** Insurance companies could utilize this information to refine their risk assessment models. They might offer specific health insurance packages or discounts to females, considering the higher likelihood of cardiovascular diseases.\n",
        "\n",
        "**Potential Negative Impact:**\n",
        "\n",
        "**Health Disparities:** If not addressed properly, this disparity in cardiovascular disease prevalence between genders could contribute to existing health disparities. Businesses and policymakers need to ensure that this information does not lead to discrimination in access to healthcare or insurance based on gender.\n",
        "\n",
        "**Increased Healthcare Costs:** For businesses providing health insurance to their employees, a higher prevalence of cardiovascular disease, especially among females, could lead to increased healthcare costs. This might result in higher premiums for health insurance plans, affecting both employers and employees.\n",
        "\n"
      ],
      "metadata": {
        "id": "ffkbRdex9qO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart-3\n",
        "## Distribution of number of people with/without cardiovascular disease of Males and Females for the different age groups in count plot??\n",
        "\n"
      ],
      "metadata": {
        "id": "lDK2VJ5GiFxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Set the figure size for the visualization\n",
        "rcParams['figure.figsize'] = 14, 6\n",
        "\n",
        "# Create a count plot using Seaborn\n",
        "sns.set(style=\"whitegrid\")  # Set the style of the plot\n",
        "plot = sns.countplot(x='Age', hue='Presence_or_absence_of_cardiovascular_disease', data=hd_df, palette=\"Set2\")\n",
        "\n",
        "# Customize the plot for better understanding\n",
        "plt.title('Distribution of Cardiovascular Disease Across Age Groups', fontsize=16, fontweight='bold')  # Add a title to the plot\n",
        "plt.xlabel('Age', fontsize=14, fontweight='bold')  # Label for X-axis\n",
        "plt.ylabel('Count', fontsize=14, fontweight='bold')  # Label for Y-axis\n",
        "\n",
        "# Set x-axis tick labels to integer data type\n",
        "plot.set_xticklabels(plot.get_xticks(), rotation=45, fontsize=12)  # Rotate x-axis labels for better visibility\n",
        "\n",
        "# Customize legend for better interpretation.\n",
        "legend_labels = ['No Cardiovascular Disease', 'Cardiovascular Disease']\n",
        "plt.legend(title='Presence or Absence of Cardiovascular Disease',labels=legend_labels, title_fontsize='x-large', fontsize='large')\n",
        "\n",
        "# Add labels to the bars for detailed information\n",
        "for p in plot.patches:\n",
        "    plot.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                  ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_fKOMx2Zadzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "The count plot using Seaborn to visualize the distribution of cardiovascular disease across different age groups. The specific chart chosen here is a count plot because it effectively displays the count of observations in each age group while differentiating between individuals with cardiovascular disease and those without it.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "**Insights from the Chart:**\n",
        "**Prevalence Across Age Groups:**\n",
        "\n",
        "The chart provides a clear view of how the prevalence of cardiovascular disease varies across different age groups.\n",
        "It appears that the incidence of cardiovascular disease generally increases with age. This aligns with the common understanding that the risk of cardiovascular issues tends to rise as individuals grow older.\n",
        "**Higher Risk in Older Age:**\n",
        "\n",
        "There is a significant increase in the count of individuals with cardiovascular disease in the older age groups, indicating that older individuals are more likely to have cardiovascular issues compared to younger age groups.\n",
        "**No Cardiovascular Disease Dominates in Younger Age:**\n",
        "\n",
        "In the younger age groups, there are noticeably more individuals without cardiovascular disease compared to those with the disease. This suggests that cardiovascular problems are relatively uncommon in younger populations.\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "Each bar represents the count of individuals in a specific age group, segmented by their cardiovascular disease status.\n",
        "\n",
        "**Potential Positive Business Impacts:**\n",
        "\n",
        "\n",
        "**Targeted Marketing and Product Development:** Understanding which age groups are more prone to cardiovascular diseases can aid businesses in creating targeted marketing campaigns for specific age demographics. Additionally, it can inform the development of products or services catering to the health needs of these age groups.\n",
        "\n",
        "Healthcare Services: Hospitals, clinics, and healthcare providers can use this information to improve their services for specific age groups. They can offer preventive health check-ups and early screening programs for the age groups most at risk.\n",
        "\n",
        "**Insurance and Wellness Programs:** Insurance companies can design specialized insurance plans or wellness programs targeting individuals in age groups with higher cardiovascular disease prevalence. This targeted approach can lead to better customer engagement and satisfaction.\n",
        "\n",
        "**Research and Development:** Pharmaceutical companies and research institutions can utilize this information to focus their research efforts on developing medications or therapies that specifically address cardiovascular issues prevalent in certain age groups.\n",
        "\n",
        "**Potential Negative Business Impacts:**\n",
        "Increased Healthcare Costs: If the data shows a significant prevalence of cardiovascular diseases in a particular age group, it could indicate higher healthcare costs for insurance providers and individuals within that demographic.\n",
        "\n",
        "**Decreased Workforce Productivity:** If a specific age group within the workforce is more affected by cardiovascular diseases, it might lead to decreased productivity, increased sick leaves, and potential early retirements, impacting businesses relying heavily on that demographic.\n",
        "\n",
        "**Challenges in Insurance Industry:** Insurance companies might face challenges in providing affordable coverage to individuals in age groups with higher cardiovascular risks, potentially leading to a reduction in the number of insured individuals."
      ],
      "metadata": {
        "id": "wjJoXDxO9mgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 4\n",
        "## Distribution of Heart rate measure of Males and Females for the different age groups in line plot??"
      ],
      "metadata": {
        "id": "jhVj6clntYCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Calculate the mean heart rate measure for males.\n",
        "mean_heart_rate_male = hd_df[hd_df['gender'] == 'M']['Heart_Rate_measure'].mean()\n",
        "\n",
        "# Print the mean heart rate measure for males.\n",
        "print(\"Mean Heart Rate Measure for Males: {:.2f}\".format(mean_heart_rate_male))"
      ],
      "metadata": {
        "id": "xOYjCrzgrfgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group and aggregate the data\n",
        "choletsrl_data = hd_df.groupby(['gender','Age'])['Heart_Rate_measure'].mean().reset_index()\n",
        "\n",
        "# Create pivot table\n",
        "pivot_table = choletsrl_data.pivot(index='Age', columns='gender', values='Heart_Rate_measure')\n",
        "\n",
        "# Create the plot\n",
        "pivot_table.plot(kind='line', marker='o', figsize=(15, 5),color=sns.color_palette('bright',12))\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Age',size=15, fontweight='bold')\n",
        "plt.ylabel('Heart Rate measure',size=15, fontweight='bold')\n",
        "plt.title('Distribution of Males and Females having Heart Rate measure at different ages ',size=15, fontweight='bold')\n",
        "\n",
        "#plt.plot(x, y1, label='Line 1')  # Label for the first line\n",
        "\n",
        "# Customize legend for better interpretation.\n",
        "legend_labels = ['Females', 'Males']\n",
        "plt.legend(title='Gender',labels=legend_labels, title_fontsize='x-large', fontsize='large')\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBM9NQ5JULuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        " Line charts are excellent for comparing trends over a continuous variable (in this case, age). By plotting the heart rate measures over age, we can easily compare how the heart rate measures change for males and females as they age.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The provided code creates a line chart to visualize the distribution of heart rate measures for males and females at different ages:\n",
        "\n",
        "**Trend of Heart Rate with Age:**\n",
        "\n",
        "The chart shows how average heart rate measures change with age for both males and females.\n",
        "we can observe whether heart rate generally increases, decreases, or remains stable with age for both genders.\n",
        "**Gender Differences:**\n",
        "\n",
        "By comparing the lines for males and females, you can identify any significant differences in heart rate between genders across various age groups.\n",
        "For example, if one gender consistently has a higher or lower heart rate than the other, it could indicate a gender-specific trend.\n",
        "\n",
        "**Variability and Outliers:**\n",
        "\n",
        "Variability in heart rate measures can be observed through the spread of data points around the lines. If there are wide variations, it suggests a diverse range of heart rate measures within each age group.\n",
        "Outliers, if present, can also be identified. These are data points significantly different from the rest, indicating potential anomalies in heart rate data.\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        " Analyzing this data visualization can potentially provide valuable insights into cardiovascular health patterns among different age groups and genders. Let's consider the possible positive business impacts, negative growth insights, and justifications for both scenarios:\n",
        "\n",
        "**Potential Positive Business Impacts:**\n",
        "\n",
        "**Healthcare Services and Products:**\n",
        "\n",
        "**Targeted Services:** With insights into average heart rates across age and gender groups, healthcare providers can develop targeted services and products. For example, specialized health checkups or heart rate monitoring devices tailored for specific age and gender groups.\n",
        "**Preventive Care:** Businesses can create preventive care programs and awareness campaigns aimed at specific demographics, encouraging healthier lifestyles and early detection of cardiovascular issues.\n",
        "Pharmaceuticals:\n",
        "\n",
        "**Medication Development:** Pharmaceutical companies can use this data to develop medications that are specifically tailored for certain age and gender groups, potentially leading to more effective treatments and medications.\n",
        "Fitness and Wellness Industry:\n",
        "\n",
        "**Fitness Programs:** Gyms and fitness centers can design age and gender-specific fitness programs. Understanding average heart rates can help in creating workout routines that are safe and effective for different demographics.\n",
        "\n",
        "**Insurance Companies:**\n",
        "\n",
        "**Risk Assessment:** Insurance companies can refine their risk assessment models based on gender and age, potentially leading to more accurate underwriting and pricing strategies for health insurance policies.\n",
        "Potential Negative Growth Insights:\n",
        "Healthcare Disparities:\n",
        "\n",
        "**Identifying Disparities:** If there are significant disparities in heart rates among different age and gender groups, it could indicate underlying health disparities in the population. This might require interventions to address these gaps in healthcare access and quality.\n",
        "Public Health Concerns:\n",
        "\n",
        "**Potential Health Issues:** Unusually high or low heart rates in specific demographic groups could indicate potential health issues or risk factors. If these issues are not addressed, it could lead to a negative impact on public health and, consequently, economic productivity.\n",
        "Medical Costs:\n",
        "\n",
        "**Increased Healthcare Costs:** If certain demographic groups show consistently higher heart rates, it might indicate a higher prevalence of cardiovascular diseases. This could lead to increased healthcare costs for both individuals and the healthcare system, potentially impacting economic growth.\n",
        "In summary, the insights gained from analyzing the heart rate data can indeed lead to positive business impacts by enabling targeted healthcare services, tailored products, and improved risk assessment strategies. However, it's crucial to address any negative growth insights promptly, focusing on interventions, awareness programs, and policies to mitigate disparities and potential health issues. Businesses and policymakers can collaborate to ensure that the insights derived from such data lead to positive outcomes for public health and the economy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZbUX-CU9pnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart-5\n",
        "## Distribution of number of people with/without cardiovascular disease of Males and Females for the different age groups in line plot??"
      ],
      "metadata": {
        "id": "TdKZ5-oU0cVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Group and aggregate the data\n",
        "data_grouped = hd_df.groupby(['gender','Age'])['Presence_or_absence_of_cardiovascular_disease'].sum().reset_index()\n",
        "\n",
        "# Create pivot table\n",
        "pivot_table = data_grouped.pivot(index='Age', columns='gender', values='Presence_or_absence_of_cardiovascular_disease')\n",
        "\n",
        "# Create the plot\n",
        "pivot_table.plot(kind='line', marker='o', figsize=(15, 5),color=sns.color_palette('bright',12))\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Age',size=14,fontweight='bold')\n",
        "plt.ylabel('Number of people having cardiovascular disease',size=12,fontweight='bold')\n",
        "plt.title('Distribution of Males and Females having cardiovascular disease at different ages',size=14,fontweight='bold')\n",
        "\n",
        "\n",
        "# Customize legend for better interpretation.\n",
        "legend_labels = ['Females', 'Males']\n",
        "plt.legend(title='Gender',labels=legend_labels, title_fontsize='x-large', fontsize='large')\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q3ekxapOSkhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "The line chart is an appropriate choice for visualizing the distribution of males and females with cardiovascular disease at different ages because it effectively communicates the trends, allows for easy comparison, and provides insights into how the disease prevalence varies with age for both genders.\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The given code creates a line chart representing the distribution of males and females with cardiovascular disease at different ages.\n",
        "\n",
        "**Age of Onset:** The chart can provide insights into the age at which males and females are more likely to develop cardiovascular diseases. Peaks or trends in the chart can indicate ages where the risk is significantly higher.\n",
        "\n",
        "**Gender Comparison:** By comparing the lines for males and females, you can observe if there are differences in the prevalence of cardiovascular diseases between genders at different ages. For example, if one line consistently stays above the other, it indicates a gender-specific trend in disease prevalence.\n",
        "\n",
        "**Age Range of Concern:** The chart can identify the specific age ranges where there is a notable increase or decrease in the number of people with cardiovascular diseases. This information is vital for healthcare professionals and policymakers to target specific age groups for preventive measures and interventions.\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The provided code groups and aggregates the data based on gender and age, then creates a line chart to visualize the distribution of males and females having cardiovascular disease at different ages.\n",
        "\n",
        "**Potential Positive Business Impact:**\n",
        "**Targeted Marketing and Prevention Programs:** If the chart shows that a particular age group and gender have a higher incidence of cardiovascular diseases, businesses and healthcare providers can tailor their marketing and prevention programs to target this demographic specifically. For example, they can promote relevant healthcare services, insurance plans, or wellness products to the at-risk population, potentially increasing sales and revenue.\n",
        "\n",
        "**Product Development:** Insights into the age and gender groups most affected by cardiovascular diseases can lead to the development of new products or services catering to the specific needs of this demographic. This targeted approach can enhance customer satisfaction and loyalty.\n",
        "\n",
        "**Healthcare Services Optimization:** Hospitals and healthcare providers can optimize their services based on the demographics most affected. For instance, if a specific age group and gender are found to have a higher incidence of cardiovascular diseases, hospitals can allocate resources, staff, and facilities accordingly to improve patient care.\n",
        "\n",
        "**Research and Development:** Pharmaceutical companies and research institutions can focus their research efforts on developing medications and treatments that cater to the predominant demographic affected by cardiovascular diseases, potentially leading to breakthroughs and new revenue streams.\n",
        "\n",
        "**Potential Negative Impact:**\n",
        "\n",
        "\n",
        "**Increased Healthcare Costs:** If the chart indicates a rise in cardiovascular diseases among a particular demographic, it could lead to increased healthcare costs for both individuals and the government. Businesses might face challenges related to employee health insurance costs and reduced productivity due to employee health issues.\n",
        "\n",
        "**Reputation Damage:** If a business is directly related to the healthcare industry and the insights from the chart reveal negative health trends, it could harm the business's reputation. Patients might avoid healthcare providers or services associated with high incidences of cardiovascular diseases.\n",
        "\n",
        "**Decreased Workforce Productivity:** Cardiovascular diseases can lead to absenteeism and reduced productivity among employees. Businesses might face challenges in maintaining an efficient workforce, impacting overall productivity and profitability.\n",
        "\n",
        "To draw specific conclusions about the positive or negative impact, it's crucial to analyze the actual chart and data. Understanding the trends, patterns, and their implications in the context of the business or industry is essential for making informed decisions that can lead to positive outcomes and mitigate potential negative impacts."
      ],
      "metadata": {
        "id": "GaCWzlx88UmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart-6\n",
        "## Age Distribution of people having Cardiovascular disease and not having Cardiovascular disease in violin plot??\n",
        "\n"
      ],
      "metadata": {
        "id": "54Q0cprwya1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Assuming hd_df is the DataFrame containing the data\n",
        "# Select rows where 'Presence_or_absence_of_cardiovascular_disease' column has a value of 1\n",
        "cardio_disease_df = hd_df[hd_df['Presence_or_absence_of_cardiovascular_disease'] == 1]\n",
        "non_cardio_disease_df = hd_df[hd_df['Presence_or_absence_of_cardiovascular_disease'] == 0]"
      ],
      "metadata": {
        "id": "3OBQk0fyiW4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the style for the plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a figure with subplots\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot 1: Age distribution in non_cardio_disease_df\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.violinplot(x=non_cardio_disease_df.Age, color='orange', orient='h')\n",
        "plt.title('Age Distribution in Non-Cardiovascular diseases Cases')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('')\n",
        "\n",
        "# Add statistical information if desired\n",
        "# mean_age = hd_df.Age.mean()\n",
        "# median_age = hd_df.Age.median()\n",
        "# plt.axvline(mean_age, color='blue', linestyle='dashed', linewidth=2, label=f'Mean Age: {mean_age:.2f}')\n",
        "# plt.axvline(median_age, color='green', linestyle='dashed', linewidth=2, label=f'Median Age: {median_age}')\n",
        "\n",
        "# Print observations\n",
        "print(\"Observations have been recorded mostly for people aged between 38 and 55 in non-cardiovascular cases.\")\n",
        "\n",
        "# Plot 2: Age distribution in cardio_disease_df\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.violinplot(x=cardio_disease_df.Age, color='red', orient='h')\n",
        "plt.title('Age Distribution in Cardiovascular Cases')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('')\n",
        "\n",
        "# Add statistical information if desired\n",
        "# mean_age = cardio_disease_df.Age.mean()\n",
        "# median_age = cardio_disease_df.Age.median()\n",
        "# plt.axvline(mean_age, color='blue', linestyle='dashed', linewidth=2, label=f'Mean Age: {mean_age:.2f}')\n",
        "# plt.axvline(median_age, color='green', linestyle='dashed', linewidth=2, label=f'Median Age: {median_age}')\n",
        "\n",
        "# Print observations\n",
        "print(\"Observations have been recorded mostly for people with cardiovascular disease between the ages of 48 and 65.\")\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OSGchqa9RYeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "A violin plot to visualize the distribution of ages in two subsets of the dataset: cases with cardiovascular disease and cases without cardiovascular disease (assuming hd_df represents cases without cardiovascular disease and cardio_disease_df represents cases with cardiovascular disease). Violin plots are used to display the distribution and probability density of numeric data across different categories or groups. In this case, the chart is displaying the age distribution for two specific groups: those with cardiovascular disease and those without cardiovascular disease.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "We are creating violin plots to visualize the age distribution in two different datasets: hd_df representing non-cardiovascular cases and cardio_disease_df representing cardiovascular cases. Here are the insights that can be drawn from the charts and the comments provided in the code:\n",
        "\n",
        "**Non-Cardiovascular Cases (Left Plot - Orange Violin Plot):**\n",
        "\n",
        "The age distribution for non-cardiovascular cases (represented by the orange violin plot) shows that observations have been recorded mostly for people aged between 38 and 55.\n",
        "This group represents individuals without cardiovascular diseases.\n",
        "**Cardiovascular Cases (Right Plot - Red Violin Plot):**\n",
        "\n",
        "The age distribution for cardiovascular cases (represented by the red violin plot) indicates that observations have been recorded mostly for people with cardiovascular diseases between the ages of 48 and 65.\n",
        "This group represents individuals diagnosed with cardiovascular diseases.\n",
        "\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "The given code generates two violin plots comparing the age distribution between cases with cardiovascular disease and non-cardiovascular cases. Before discussing the positive and negative business impacts, let's analyze the insights gained from the plots and the provided observations:\n",
        "\n",
        "**Non-Cardiovascular Cases (Left Plot):**\n",
        "\n",
        "**Insight:** Most observations are recorded for people aged between 38 and 55 in non-cardiovascular cases.\n",
        "\n",
        "\n",
        "**Potential Positive Impact:** Understanding the age group of people without cardiovascular disease can help in targeted marketing for preventive healthcare services, fitness products, and lifestyle-related interventions. Businesses can tailor their products and services to cater to the needs of this demographic, potentially leading to increased sales and positive business impact.\n",
        "\n",
        "**Cardiovascular Cases (Right Plot):**\n",
        "\n",
        "**Insight:** Observations have been recorded mostly for people with cardiovascular disease between the ages of 48 and 65.\n",
        "Potential Negative Impact: This insight indicates a higher incidence of cardiovascular diseases in the age group of 48 to 65. From a business perspective, this could imply a potential market for pharmaceuticals, healthcare services, or specialized treatments targeting this age group. However, it also highlights a negative aspect - the prevalence of cardiovascular diseases in this age group, indicating a higher demand for healthcare services and potentially higher healthcare costs for individuals and insurance providers.\n",
        "\n",
        "**Positive Business Impact:**\n",
        "\n",
        "**Targeted Marketing:** Businesses can target the age group of 38-55 with products and services promoting healthy lifestyles, fitness equipment, and preventive healthcare services.\n",
        "\n",
        "**Healthcare Industry Opportunities:** Pharmaceutical companies, healthcare providers, and insurance companies can develop specialized services and products for the age group of 48-65, creating business opportunities in the healthcare sector.\n",
        "Negative Impact:\n",
        "\n",
        "**Healthcare Costs:** The higher incidence of cardiovascular diseases in the age group of 48-65 indicates potential negative consequences for individuals and insurance providers due to increased healthcare costs. Businesses in the healthcare sector might face challenges in managing the costs associated with treating cardiovascular diseases.\n",
        "\n",
        "while there are opportunities for businesses in the healthcare industry to address the specific needs of the identified age groups, the prevalence of cardiovascular diseases in the older age group also indicates potential challenges and increased costs in the healthcare sector. Businesses need to consider both the opportunities and challenges when strategizing their products and services, emphasizing preventive measures and innovative healthcare solutions to mitigate the negative impacts associated with the prevalence of cardiovascular diseases.\n"
      ],
      "metadata": {
        "id": "6fLMfMLjkhNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 7\n",
        "## Distribution of Cholesterol measure of Males and Females for the different age groups in line plot??\n",
        "\n"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Group and aggregate the data\n",
        "choloestrl_data = hd_df.groupby(['gender','Age'])['Cholesterol_measure'].mean().reset_index()\n",
        "\n",
        "# Create pivot table\n",
        "pivot_table = choloestrl_data.pivot(index='Age', columns='gender', values='Cholesterol_measure')\n",
        "\n",
        "# Create the plot\n",
        "pivot_table.plot(kind='line', marker='o', figsize=(15, 5),color=sns.color_palette('bright',12))\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Age',size=14,fontweight='bold')\n",
        "plt.ylabel('Cholesterol measure',size=14,fontweight='bold')\n",
        "plt.title('Distribution of Males and Females having Cholesterol measure at different ages',size=14,fontweight='bold')\n",
        "\n",
        "# Customize legend for better interpretation.\n",
        "legend_labels = ['Females', 'Males']\n",
        "plt.legend(title='Gender',labels=legend_labels, title_fontsize='x-large', fontsize='large')\n",
        "\n",
        "# Display the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RPpVJgL5TZxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "A line chart to visualize the distribution of cholesterol measures in males and females at different ages. This specific chart, a line chart, was chosen likely due to the nature of the data and the research question being addressed. Here's why a line chart might be appropriate for this scenario:\n",
        "\n",
        "**Temporal Relationship:** Line charts are excellent for displaying data points in chronological order or in this case, in ascending age order. This chart seems to show how cholesterol measures change with age for both males and females, making a line chart a suitable choice.\n",
        "\n",
        "**Comparison of Trends:** Line charts are effective for comparing trends over a continuous variable (in this case, age) between different categories (males and females). The lines for males and females can be compared easily, allowing viewers to discern patterns and differences in cholesterol measures as age increases.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "The above line chart comparing the average cholesterol measures between males and females at different ages. Let's analyze the potential insights that can be derived from this chart:\n",
        "\n",
        "**Cholesterol Trends with Age:**\n",
        "\n",
        "The chart allows you to observe how average cholesterol measures change with age for both males and females. You can identify if there are specific age ranges where cholesterol levels tend to increase or decrease for either gender.\n",
        "\n",
        "**Gender Differences:**\n",
        "\n",
        "By comparing the lines representing males and females, you can determine if there are significant differences in cholesterol measures between the genders. For example, you might find that females have higher cholesterol levels than males in certain age groups or vice versa.\n",
        "**Critical Age Groups:**\n",
        "\n",
        "Peaks or valleys in the lines could indicate critical age groups where cholesterol levels spike or drop. Identifying these age groups is essential for understanding when individuals might be at higher risk for cardiovascular issues related to cholesterol.\n",
        "\n",
        "**Gender Disparities:**\n",
        "\n",
        "If there are consistent gaps between the lines, it suggests a consistent disparity in cholesterol levels between males and females across different ages. This insight could be crucial for health policymakers and practitioners to design targeted interventions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "The provided code generates a line chart showing the distribution of cholesterol measures for males and females at different ages. It calculates the average cholesterol measure for each gender at various ages and presents the data in a line chart format.\n",
        "\n",
        "Regarding the impact on business decisions, the insights gained from this chart can indeed be valuable. Here's how:\n",
        "\n",
        "**Positive Business Impact:**\n",
        "\n",
        "**Targeted Marketing:** If there are age groups where both males and females consistently show higher cholesterol measures, companies related to healthcare, pharmaceuticals, or health insurance can target these demographics with relevant products and services. For example, they could market cholesterol-lowering medications, health supplements, or health check-up packages.\n",
        "\n",
        "**Health and Wellness Programs:** Employers and health insurance companies could use this information to design targeted health and wellness programs. For instance, they could offer cholesterol screening and awareness campaigns for specific age groups, leading to healthier employees and reduced healthcare costs.\n",
        "\n",
        "**Product Development:** Companies in the pharmaceutical sector might invest in research and development of new drugs or treatments for managing cholesterol, especially if certain age groups consistently show higher cholesterol levels.\n",
        "\n",
        "**Potential Negative Growth:**\n",
        "\n",
        "\n",
        "**Increased Healthcare Costs:** If there are specific age groups where both genders show a consistent rise in cholesterol measures, this could lead to increased healthcare costs. Insurance companies might see higher claims from individuals within these age groups, potentially impacting their profitability.\n",
        "\n",
        "**Public Health Concerns:** From a broader perspective, consistently high cholesterol levels in specific age groups could indicate a public health concern. If not addressed, this might lead to an increase in the prevalence of cardiovascular diseases, which could strain healthcare resources and negatively impact overall societal health.\n",
        "\n",
        "On the other hand, if the high cholesterol levels are due to genetic factors, businesses could focus on developing targeted medications and therapies to address these specific needs, turning a potential negative impact into a positive one by providing solutions to a pressing health issue.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9tu27bmAmySj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart-8\n",
        "## Bar plot for cardiac risk disease for different cholesterol level.\n",
        "\n"
      ],
      "metadata": {
        "id": "axsjBcSw10Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Define the bin edges for categorization\n",
        "bins = [113, 200, 400, 600]  # Adjust the bin edges as needed\n",
        "\n",
        "# Define the corresponding labels for the categories\n",
        "labels = ['Normal', 'Above Normal', 'Well Above Normal']\n",
        "\n",
        "# Use pd.cut to categorize the 'Cholesterol' column\n",
        "hd_df['Cholesterol Category'] = pd.cut(hd_df['Cholesterol_measure'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Group the data by 'Cholesterol Category' and calculate the mean of 'Presence_or_absence_of_cardiovascular_disease'\n",
        "age_chol = pd.DataFrame({\n",
        "    'Cholesterol Category': labels,\n",
        "    'Cardiac Disease Risk': hd_df.groupby('Cholesterol Category')['Presence_or_absence_of_cardiovascular_disease'].mean()\n",
        "})\n",
        "\n",
        "# Create a bar plot using Plotly Express\n",
        "fig = px.bar(age_chol, x='Cholesterol Category', y='Cardiac Disease Risk',\n",
        "             color='Cholesterol Category', title='Cardiac Disease Risk by Cholesterol Level')\n",
        "\n",
        "# Customize the appearance of the plot\n",
        "fig.update_traces(texttemplate='%{y:.2f}', textposition='outside')\n",
        "fig.update_layout(title_text='Cardiac Disease Risk by Cholesterol Level', title_x=0.5)  # Centered title\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "RFiB4rJpg7Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The specific chart chosen for this analysis is a bar chart. This choice is appropriate for several reasons:\n",
        "\n",
        "**Categorical Data:** The analysis involves categorizing cholesterol levels into discrete groups: 'Normal', 'Above Normal', and 'Well Above Normal'. Bar charts are excellent for displaying and comparing categorical data.\n",
        "\n",
        "**Comparison of Categories:** Bar charts are great for comparing different categories or groups. In this case, the chart compares the risk of cardiovascular disease across different cholesterol categories.\n",
        "\n",
        "**Easy Interpretation:** Bar charts are easy to understand and interpret, making them suitable for conveying the relationship between cholesterol levels and the risk of cardiovascular disease to a wide audience.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The provided code categorizes individuals into different cholesterol levels (Normal, Above Normal, and Well Above Normal) based on the 'Cholesterol_measure' column in the dataset. It then calculates the mean of 'Presence_or_absence_of_cardiovascular_disease' for each cholesterol category and creates a bar chart using Plotly Express to visualize the cardiac disease risk associated with these cholesterol levels.\n",
        "\n",
        "**Insights from the Chart:**\n",
        "\n",
        "**Cardiac Disease Risk Increases with Higher Cholesterol Levels:**\n",
        "\n",
        "The chart likely shows that individuals with higher cholesterol levels (Above Normal and Well Above Normal) have a higher mean probability of having cardiovascular diseases compared to those with Normal cholesterol levels. This suggests a positive correlation between cholesterol levels and the risk of cardiovascular diseases.\n",
        "\n",
        "\n",
        "**Clear Categorization of Risk Levels:**\n",
        "\n",
        "By categorizing cholesterol levels into distinct groups (Normal, Above Normal, and Well Above Normal), the chart provides a clear and easy-to-understand representation of how different cholesterol levels relate to the risk of cardiovascular diseases. This categorization helps in identifying thresholds beyond which the risk significantly increases.\n",
        "\n",
        "**Potential Threshold Identification:**\n",
        "\n",
        "The chart might help in identifying specific cholesterol level thresholds where the risk of cardiovascular diseases starts to significantly rise. This information can be valuable for healthcare professionals to set guidelines for patients regarding cholesterol management.\n",
        "\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "Analyzing the impact of cholesterol levels on cardiovascular disease risk using the provided chart can indeed provide valuable insights for a business, especially if the business is related to healthcare, insurance, or wellness services. Let's evaluate potential positive and negative impacts based on the insights gained from the chart:\n",
        "\n",
        "**Positive Business Impact:**\n",
        "\n",
        "**Targeted Marketing and Product Development:** If the chart shows a clear trend that individuals with higher cholesterol levels have a significantly higher risk of cardiovascular diseases, businesses in the healthcare industry can target these individuals with specific products and services. This could include cholesterol-lowering medications, specialized diet plans, exercise programs, or regular health check-ups.\n",
        "\n",
        "**Insurance and Health Services:** Insurance companies can use this information to refine their risk assessment models. Individuals with well above normal cholesterol levels might be charged higher premiums, creating a new revenue stream. Additionally, health services can be tailored to focus on preventive care for individuals with high cholesterol.\n",
        "\n",
        "**Educational Campaigns:** Businesses can run educational campaigns to raise awareness about the risks associated with high cholesterol. This can lead to increased sales of health foods, gym memberships, and other wellness products and services.\n",
        "\n",
        "**Partnerships and Collaborations:** Companies producing cholesterol-lowering medications or health foods might collaborate with healthcare providers to offer bundled services, creating a win-win situation for both businesses.\n",
        "\n",
        "**Negative Business Impact:**\n",
        "\n",
        "\n",
        "**Stigmatization Concerns:** Charging higher premiums or targeting specific products towards individuals with high cholesterol might lead to stigmatization or dissatisfaction among customers. This could harm the company's reputation and customer trust.\n",
        "\n",
        "**Limited Market Scope:** Focusing solely on individuals with high cholesterol might limit the market scope. Businesses should be careful not to exclude individuals with normal cholesterol levels, as they also need healthcare products and services.\n",
        "\n",
        "In summary, while the insights gained from the provided chart can lead to positive business impacts, there are potential negative consequences related to ethics, stigmatization, and regulatory compliance. Businesses must strike a balance between capitalizing on these insights and acting responsibly and ethically to ensure a positive impact on society and their bottom line.\n"
      ],
      "metadata": {
        "id": "LiSDgBujmwq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 9\n",
        "## Bar plot for cardiac risk disease based on daily cigarettes consumption.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Define the bin edges for categorization\n",
        "bins = [0, 1, 6, 15, 70]  # Adjust the bin edges as needed\n",
        "\n",
        "# Define the corresponding labels for the categories\n",
        "labels = ['Do not smoke','Normal', 'Above Normal', 'Well Above Normal']\n",
        "\n",
        "# Use pd.cut to categorize the 'Cholesterol' column\n",
        "hd_df['Cigrets_smoked_per_day_category'] = pd.cut(hd_df['Cigrets_smoked_per_day'], bins=bins, labels=labels, right=False)"
      ],
      "metadata": {
        "id": "D0bdjkep1bae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'Cigarettes_smoked_per_day_category' and calculate the mean of 'Presence_or_absence_of_cardiovascular_disease'\n",
        "cigarette_groups = hd_df.groupby('Cigrets_smoked_per_day_category')['Presence_or_absence_of_cardiovascular_disease'].mean()\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "cigarette_data = pd.DataFrame({\n",
        "    'Cardiac Disease Risk': cigarette_groups.values,\n",
        "    'Cigarettes Smoked per Day': ['Do not smoke(0-1)', 'Normal(1-6)', 'Above Normal(6-15)', 'Well Above Normal(15-70)']\n",
        "})\n",
        "\n",
        "# Create a bar plot using Plotly Express\n",
        "fig = px.bar(cigarette_data,\n",
        "             x='Cigarettes Smoked per Day',\n",
        "             y='Cardiac Disease Risk',\n",
        "             color='Cigarettes Smoked per Day')\n",
        "\n",
        "# Annotate the bars with risk percentages\n",
        "for i, row in cigarette_data.iterrows():\n",
        "    risk_percentage = row['Cardiac Disease Risk']\n",
        "    fig.add_annotation(\n",
        "        x=row['Cigarettes Smoked per Day'],\n",
        "        y=risk_percentage,\n",
        "        text=f'Risk: {risk_percentage:.2%}',  # Display risk as percentage\n",
        "        showarrow=True,\n",
        "        arrowhead=2,\n",
        "        arrowcolor='black'\n",
        "    )\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    title_text='Cardiac Disease Risk Based on Daily Cigarette Consumption',  # Updated title\n",
        "    title_x=0.5,  # Center the title\n",
        "    xaxis_title='Number of Cigarettes Smoked per Day',\n",
        "    yaxis_title='Cardiac Disease Risk',\n",
        "    yaxis_ticksuffix='%'\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "qjcaM6x8h59B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The chart you've created is a bar chart displaying the cardiac disease risk based on daily cigarette consumption. In this chart, the x-axis represents the categories of cigarette consumption ('Do not smoke', 'Normal', 'Above Normal', 'Well Above Normal'), the y-axis represents the cardiac disease risk (expressed as a percentage), and each bar represents a category of cigarette consumption. Annotations on the bars provide the exact risk percentage corresponding to each category.\n",
        "\n",
        "The choice of a bar chart for this visualization is appropriate for several reasons:\n",
        "\n",
        "**Comparison of Discrete Categories:** Bar charts are excellent for comparing discrete categories or groups of data. In this case, you're comparing different levels of cigarette consumption, making a bar chart an intuitive choice.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "**Insights from the Chart:**\n",
        "\n",
        "**Risk Variation with Cigarette Consumption:**\n",
        "\n",
        "The chart provides a clear visual representation of how cardiac disease risk increases with the number of cigarettes smoked per day.\n",
        "Individuals who do not smoke have the lowest cardiac disease risk, while those categorized as 'Well Above Normal' in daily cigarette consumption have the highest risk.\n",
        "\n",
        "**Gradual Increase in Risk:**\n",
        "\n",
        "There seems to be a gradual increase in cardiac disease risk as the number of cigarettes smoked per day rises. This suggests a dose-response relationship between smoking and the risk of cardiovascular diseases.\n",
        "\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "'Presence_or_absence_of_cardiovascular_disease' based on the number of cigarettes smoked per day and then visualizes this data using a bar chart. The chart aims to show the cardiac disease risk associated with different levels of daily cigarette consumption.\n",
        "\n",
        "### Positive Business Impact:\n",
        "\n",
        "1. **Smoking Cessation Programs:** If the analysis shows a significant increase in cardiac disease risk for individuals who smoke more, this insight can be used by businesses and health organizations to emphasize the importance of smoking cessation programs. Companies offering such programs can target heavy smokers and potentially reduce the risk of cardiovascular diseases among their employees.\n",
        "\n",
        "2. **Health Insurance and Wellness Programs:** Insurance companies and employers providing health insurance can utilize this information to encourage healthier lifestyles among policyholders and employees. This insight might lead to the development of wellness programs specifically tailored towards reducing smoking habits, potentially leading to better health outcomes and lower insurance claims related to cardiovascular diseases.\n",
        "\n",
        "3. **Pharmaceutical and Healthcare Industries:** Pharmaceutical companies might find this data valuable for developing smoking cessation medications. Additionally, healthcare providers can use this information to counsel patients about the health risks associated with smoking, potentially leading to increased patient engagement and compliance with prescribed treatments.\n",
        "\n",
        "### Negative Business Impact:\n",
        "\n",
        "1. **Increased Healthcare Costs:** If a large number of individuals in a workforce or customer base are heavy smokers, there might be an increase in healthcare costs due to a higher prevalence of cardiovascular diseases. This could impact businesses providing health insurance to their employees or customers, leading to higher premiums or increased claims, potentially affecting the company's bottom line negatively.\n",
        "\n",
        "2. **Reduced Workforce Productivity:** Employees with cardiovascular diseases might experience reduced productivity due to health-related issues, leading to increased absenteeism or presenteeism (being present at work but not fully productive). This can impact the overall productivity and efficiency of a business.\n",
        "\n",
        "3. **Potential Legal and Ethical Issues:** In some jurisdictions, businesses might face legal or ethical challenges if they are aware of the health risks among their employees (such as high rates of cardiovascular diseases due to smoking) but do not take appropriate actions to mitigate these risks, such as providing smoking cessation programs or health education.\n",
        "\n",
        "In summary, the insights gained from the analysis can have both positive and negative impacts on businesses. Utilizing these insights to promote healthier lifestyles, develop targeted interventions, and encourage smoking cessation can lead to positive outcomes. However, ignoring these insights or failing to address the associated risks might result in negative consequences, such as increased healthcare costs and reduced workforce productivity. Business strategies should be designed with a focus on employee well-being and health to mitigate potential negative impacts.\n"
      ],
      "metadata": {
        "id": "SO31lIo_6DRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chart - 10**\n",
        "### **Correlation Heatmap**\n"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization code\n",
        "\n",
        "# Columns to drop from the DataFrame for correlation analysis\n",
        "columns_to_drop = ['id',]\n",
        "\n",
        "# Create a new DataFrame by dropping specified columns\n",
        "filtered_df = hd_df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Calculate the correlation matrix for the filtered DataFrame\n",
        "correlation_matrix = filtered_df.corr()\n",
        "\n",
        "# Get the index of top correlated features from the correlation matrix\n",
        "top_corr_features = correlation_matrix.index\n",
        "\n",
        "# Set the size of the heatmap figure\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Plot a heatmap of correlations between the top features\n",
        "# Use Seaborn's heatmap function ('sns.heatmap') with annotations and a specific color map ('RdYlGn')\n",
        "heatmap = sns.heatmap(filtered_df[top_corr_features].corr(), annot=True, cmap='RdYlGn')\n",
        "\n",
        "# Set the title and labels for the heatmap\n",
        "heatmap.set_title('Correlation Heatmap', fontsize=18)\n",
        "plt.xlabel('Features', fontsize=14)\n",
        "plt.ylabel('Features', fontsize=14)\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M83VZY4bUmwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The code you provided creates a correlation heatmap to visualize the correlations between different features in a dataset. This visualization is commonly used in data analysis to understand how variables are related to each other.\n",
        "\n",
        "#### **2. What is/are the insight(s) found from the chart?**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "\n",
        "The provided code snippet generates a correlation heatmap based on the specified dataset, highlighting the relationships between different features. Correlation heatmaps are excellent tools for visualizing correlations between variables, especially in datasets with multiple features.\n",
        "\n",
        "**Strength of Correlation:**\n",
        "\n",
        "The heatmap shows how strongly different features are correlated with each other. Positive values (closer to 1) indicate a positive correlation, while negative values (closer to -1) indicate a negative correlation.\n",
        "Feature Relationships:\n",
        "\n",
        "Features with a strong positive correlation (close to 1) suggest that they increase or decrease together.\n",
        "\n",
        "#### **3. Will the gained insights help creating a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "\n",
        "\n",
        "#### **Answer Here.**\n",
        "\n",
        "The correlation matrix through a heatmap visualization can provide valuable insights into relationships between different variables in the dataset. Positive correlations (values close to 1) indicate that as one variable increases, the other variable tends to increase as well. Negative correlations (values close to -1) indicate that as one variable increases, the other variable tends to decrease. Correlations close to 0 suggest a weak or no linear relationship between variables.\n",
        "\n",
        "**Positive Business Impacts:**\n",
        "\n",
        "**Identifying Positive Correlations:**\n",
        "\n",
        "Positive correlations between lifestyle factors (e.g., physical activity, healthy diet) and cardiovascular health indicators (e.g., lower cholesterol levels, healthy blood pressure) could suggest that promoting healthy lifestyles among customers might lead to positive health outcomes. Businesses in the health and wellness industry could leverage this insight for marketing strategies, promoting fitness products, and health-related services.\n",
        "\n"
      ],
      "metadata": {
        "id": "4_E_kEE-68pA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chart -11**\n",
        "## **Pair Plot**"
      ],
      "metadata": {
        "id": "sMhva2pPU21a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot\n",
        "sns.pairplot(hd_df, hue=\"Presence_or_absence_of_cardiovascular_disease\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KBXh3KliU0V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "SvKYj-BW9Roh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here.**\n",
        "\n",
        "A pairplot, often referred to as a scatterplot matrix, serves as a visualization method enabling the examination of connections between every pair of variables within a dataset. This visualization tool proves valuable for data exploration as it provides a swift and comprehensive means to discern the interrelationships among all variables present in the dataset.\n",
        "\n",
        "Therefore, we employed a pair plot to scrutinize data patterns and unveil the associations among different features. It essentially accomplishes the same purpose as a correlation map but does so through graphical representation, allowing for a visual exploration of the data's intricate relationships."
      ],
      "metadata": {
        "id": "IfObxZj988Zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "x__rt11z9rp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Here.**\n",
        "\n",
        "The distribution of the \"cigs_per_day\" data is significantly skewed, and it includes a substantial number of values that are equal to zero. Therefore, it may be advisable to transform this data into a categorical column."
      ],
      "metadata": {
        "id": "-LOkcJAq9r_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Null Hypothesis:-** The level of education does not appear to be linked to the outcome of coronary heart disease (CHD).\n",
        "####  **Alternate Hypothesis:-** A correlation exists between the level of education and the outcome of coronary heart disease (CHD)."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value.\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# create cotingency table.\n",
        "contingency_table = pd.crosstab(hd_df['education'] , hd_df['Presence_or_absence_of_cardiovascular_disease'])\n",
        "\n",
        "# Perform chi-squared test.\n",
        "chi1,p,dof,expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# print cotingency table.\n",
        "print(contingency_table)\n",
        "\n",
        "# print p-value.\n",
        "print(f'p-value: {p}')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The p-value is well below 0.05, leading us to reject the null hypothesis decisively."
      ],
      "metadata": {
        "id": "DEMtZkvTPv50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "H2LOolwNQ4Et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to investigate whether the 'education' column has any influence on the occurrence of chronic heart disease (CHD), I conducted a chi-squared test for independence. This statistical analysis helped me assess whether there was a meaningful connection between the level of education and the presence of CHD. By computing the chi-squared statistic and examining the associated p-value, I could draw a statistical conclusion regarding the relationship between these two variables within our dataset."
      ],
      "metadata": {
        "id": "9vFX_cC6Q3_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "zB46p-lDQ34L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for the chi-squared test of independence to examine whether the 'education' column has an impact on the occurrence of chronic heart disease (CHD). This choice was made because this statistical test is suitable for assessing the presence of a meaningful connection between two categorical variables. In our case, both education levels and CHD outcomes are categorical variables, making the chi-squared test an appropriate method.\n",
        "\n",
        "The chi-squared test operates by comparing the actual frequency distribution of data in a contingency table with the expected frequency distribution, assuming the null hypothesis is true. If there is a noteworthy disparity between the observed and expected frequencies, it implies a relationship between the two variables.\n",
        "\n",
        "In summary, I selected the chi-squared test of independence due to its widespread use and established reputation for examining the link between two categorical variables. This allowed me to draw a statistical inference regarding the relationship between education level and CHD outcome within our dataset."
      ],
      "metadata": {
        "id": "t_ZADnwQQ3sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The p-value is significantly lower than 0.05 so we reject the null hypothesis."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I conducted a chi-squared test of independence to assess whether the 'education' column has any influence on the occurrence of chronic heart disease (CHD). This statistical analysis enabled me to investigate if there's a noteworthy connection between the level of education and the outcome of CHD. By computing the chi-squared statistic and p-value, I gained the ability to draw a statistical conclusion regarding the association between these two variables in our dataset."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Already handled"
      ],
      "metadata": {
        "id": "Hqprlqiy5FLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the count of missing values in each column of the 'hd_df' DataFrame\n",
        "missing_value_counts = hd_df.isnull().sum()\n",
        "print(missing_value_counts)"
      ],
      "metadata": {
        "id": "9VVFJ70FTPlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments.\n",
        "# Already handled."
      ],
      "metadata": {
        "id": "6jiaVsVH5NlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I have used the Interquartile range(IQR) method to identift and remove outliers in the continuous columns of the dataset.I chose to use this technique because this is robust method to detect the outliers that is not affected by the presence of extreme values. The IQR is calculated as the 75th and 25th percentile of the data, and any value that falls between 25th percentile minus 1.5 times the IQR or above the 75th percentile plus 1.5 times the IQR is considered an outlier. By using this method.I was able to identify and remove outliers in a consistent and objective manner."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the mode of the 'education' column in the hd_df DataFrame.\n",
        "# The mode represents the most frequently occurring value in the 'education' column.\n",
        "v = hd_df['education'].mode()\n",
        "\n",
        "# Calculate the median of the 'Cigrets_smoked_per_day' column in the hd_df DataFrame.\n",
        "# The median is the middle value of a dataset when it is ordered from smallest to largest.\n",
        "s = hd_df['Cigrets_smoked_per_day'].median()\n",
        "# Convert the elements of array 'v' to integers using the 'astype' method.\n",
        "# This operation ensures that all elements in the array are of integer data type.\n",
        "v = v.astype(int)\n",
        "\n",
        "# Convert the elements of array 's' to integers using the 'astype' method.\n",
        "# This operation ensures that all elements in the array are of integer data type.\n",
        "s = s.astype(int)"
      ],
      "metadata": {
        "id": "1GiA8-wG-Ap9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform your categorical columns into a suitable format for computational analysis.\n",
        "hd_df = pd.get_dummies(hd_df, columns=['education'])"
      ],
      "metadata": {
        "id": "RYJ98dH6PdgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_df_copy=hd_df.copy()"
      ],
      "metadata": {
        "id": "NqqvXqVMQlIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_df_copy.rename(columns={'Weather_taking_BP_meds_or_not':'On_BP_meds',\n",
        " 'Presence_or_absence_of_cardiovascular_disease':'cardiovascular_disease',\n",
        " 'Patient_has_diabeties_or_not':'Diabeties',\n",
        " 'If_the_patient_has_a_history_of_hypertension':'Had_Hypertension',\n",
        " 'Weather_smoking_or_not':'Person_smokes',\n",
        " 'If_the_patient_has_a_history_of_stroke':'Had_stroke'},\n",
        "                inplace=True)"
      ],
      "metadata": {
        "id": "MAyeZ5m8Ktnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns=list(set(hd_df.columns)-set(continuous_variable))"
      ],
      "metadata": {
        "id": "M1o5twHQ6Pxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_variables=list(set(hd_df_copy.columns)-set(continuous_variable))"
      ],
      "metadata": {
        "id": "hywO5RqSQrR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(categorical_variables)"
      ],
      "metadata": {
        "id": "pHX_9tki2vEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.internals.blocks import Categorical\n",
        "# List of items we want to remove from the categorical_columns list\n",
        "items_to_remove = ['diff_sys_dis', 'Cholesterol Category', 'Cigrets_smoked_per_day_category','id']\n",
        "# Using list comprehension to remove specified items from the list\n",
        "categorical_variables = [col for col in categorical_variables if col not in items_to_remove]"
      ],
      "metadata": {
        "id": "zXNW86TI74Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_variables"
      ],
      "metadata": {
        "id": "BViFUZ6N-by7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the size of the plot.\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Define the number of rows and columns for subplots.\n",
        "rows = 4\n",
        "cols = 3\n",
        "count = 1\n",
        "\n",
        "# List of binary categorical columns to be visualized.\n",
        "categorical_variables\n",
        "\n",
        "# Labels for binary categories.\n",
        "Sex_label = ['Females', 'Males']\n",
        "Labels = ['No','Yes']\n",
        "# Loop through each binary categorical variable and create a subplot.\n",
        "for idx, var in enumerate(categorical_variables):\n",
        "    # Create subplots.\n",
        "    plt.subplot(rows, cols, count)\n",
        "    if var == 'gender':\n",
        "        hd_df_copy[var].value_counts().plot.pie(autopct='%1.1f%%', fontsize=12, labels=Sex_label, colors=['skyblue', 'lightcoral'])\n",
        "    else:\n",
        "        # Generate a pie chart for the current categorical variable.\n",
        "        hd_df_copy[var].value_counts().plot.pie(autopct='%1.1f%%', fontsize=12, labels=Labels, colors=['skyblue', 'lightcoral'])\n",
        "\n",
        "    # Increase the count for the next subplot.\n",
        "    count += 1\n",
        "\n",
        "# Adjust layout to prevent overlapping of subplots.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qhL6SSyh0o9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Answer Here.**\n",
        "#### Onehot encoding is used to encode the education column.All the remaining cateorical columns are binary(0/1) so no need to encode them."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Refine features to reduce the correlation between them and generate novel features.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calculate_varnc_inflasn_factr(df):\n",
        "  varnc_inflasn_factr = pd.DataFrame()\n",
        "  varnc_inflasn_factr['colmns_nam'] = df.columns\n",
        "  varnc_inflasn_factr['varnc_inflasn_factr'] = [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n",
        "\n",
        "  return(varnc_inflasn_factr)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your features thoughtfully to prevent overfitting.\n",
        "cont_var_df = pd.DataFrame(hd_df[continuous_variable])\n",
        "cont_var_df"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_varnc_inflasn_factr(hd_df[[c for c in cont_var_df]])"
      ],
      "metadata": {
        "id": "7R2kxGQapcML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column pulse_pressure and dropping systolic_bp and diastolic_bp.\n",
        "hd_df['diff_sys_dis'] = (hd_df['Systolic_BP_measure']-hd_df['Diastolic_BP_measure'])/2"
      ],
      "metadata": {
        "id": "neuVNNNUtud4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns\n",
        "hd_df.columns"
      ],
      "metadata": {
        "id": "abH3uQOqtuac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_column = hd_df['diff_sys_dis']\n",
        "\n",
        "# Concatenate the new column to 'continuous_variable'\n",
        "cont_var_df = pd.concat([hd_df , new_column], axis=1)"
      ],
      "metadata": {
        "id": "BUBmkqgdoabH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the continuous_var list\n",
        "\n",
        "continuous_variable.remove('Systolic_BP_measure')\n",
        "continuous_variable.remove('Diastolic_BP_measure')\n",
        "continuous_variable.append('diff_sys_dis')"
      ],
      "metadata": {
        "id": "qeDjgpSej0Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns"
      ],
      "metadata": {
        "id": "X0FEHSCHKAme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to be removed from the list\n",
        "columns_to_remove = ['id', 'Weather_smoking_or_not', 'Cholesterol Category', 'Cigrets_smoked_per_day_category']\n",
        "\n",
        "# Remove specified columns from the list of categorical columns\n",
        "categorical_columns = [column for column in categorical_columns if column not in columns_to_remove]\n"
      ],
      "metadata": {
        "id": "9SLPccjXKqCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_df.drop(columns=columns_to_remove,inplace=True)"
      ],
      "metadata": {
        "id": "i7jrZHYrNMcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping excess columns\n",
        "hd_df.drop(columns=['Systolic_BP_measure','Diastolic_BP_measure'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "b9DxTUAeq3Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_var_df = pd.DataFrame(hd_df[continuous_variable])"
      ],
      "metadata": {
        "id": "91-qsFHvoaXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_varnc_inflasn_factr(hd_df[[c for c in cont_var_df]])"
      ],
      "metadata": {
        "id": "TRHSZypIxXou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = hd_df[continuous_variable].corr()\n",
        "plt.figure(figsize=(25,10))\n",
        "sns.heatmap(corr,annot=True, cmap=plt.cm.Accent_r)"
      ],
      "metadata": {
        "id": "WbnwGOCAtdRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We employed the variance inflation factor (VIF) as a tool to address multicollinearity. Our analysis revealed that both systolic and diastolic blood pressure exhibited elevated VIF values. Consequently, we introduced a novel variable, known as diff_sys_dis.\n",
        "\n",
        "Furthermore, an observation was made regarding the \"is smoking\" column, which solely contained binary values indicating smoking status (yes or no). This same information was redundantly conveyed in the \"cigs per day\" column, where a value of 0 denoted non-smokers and a numeric value represented the daily cigarette consumption for smokers."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The key columns include 'age', 'sex', 'cigarettes per day', 'blood pressure medications', 'history of stroke', 'prevalent hypertension', 'diabetes status', 'total cholesterol level', 'body mass index (BMI)', 'heart rate', 'glucose level', 'ten-year coronary heart disease risk', 'education level 1', 'education level 2', 'education level 3', 'education level 4', and 'pulse pressure'. These columns encompass demographic details, behavioral patterns, existing medical conditions, and historical health data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Asymmetry in relation to the index axis.\n",
        "(hd_df[continuous_variable]).skew(axis=0)"
      ],
      "metadata": {
        "id": "CDHhXbd3FfZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skewness when applying the square root transformation.\n",
        "np.sqrt(hd_df[continuous_variable]).skew(axis=0)"
      ],
      "metadata": {
        "id": "8atlhFCJFkoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skewness after applying a logarithm base 10 transformation.\n",
        "np.log10(hd_df[continuous_variable] + 1).skew(axis=0)"
      ],
      "metadata": {
        "id": "50BukpF7Fkri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying a logarithmic transformation to a continuous variable.\n",
        "\n",
        "# Apply log transformation to the 'Age' column using np.log10\n",
        "hd_df['Age'] = np.log10(hd_df['Age'] + 1)\n",
        "\n",
        "# Apply square root transformation to the 'Cigrets_smoked_per_day' column using np.sqrt\n",
        "hd_df['Cigrets_smoked_per_day'] = np.sqrt(hd_df['Cigrets_smoked_per_day'])\n",
        "\n",
        "# Apply log transformation to the 'Cholestrol_measure' column using np.log10\n",
        "hd_df['Cholesterol_measure'] = np.log10(hd_df['Cholesterol_measure'] + 1)\n",
        "\n",
        "# Apply square root transformation to the 'Body_Mass_Index' column using np.sqrt\n",
        "hd_df['Body_Mass_Index'] = np.sqrt(hd_df['Body_Mass_Index'])\n",
        "\n",
        "# Apply log transformation to the 'Heart_Rate_measure' column using np.log10\n",
        "hd_df['Heart_Rate_measure'] = np.log10(hd_df['Heart_Rate_measure'] + 1)\n",
        "\n",
        "# Apply square root transformation to the 'glucose' column using np.sqrt\n",
        "hd_df['glucose'] = np.sqrt(hd_df['glucose'])"
      ],
      "metadata": {
        "id": "1hoDYU-6GZY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assessing the asymmetry following the application of a logarithmic transformation.\n",
        "hd_df[continuous_variable].skew(axis=0)"
      ],
      "metadata": {
        "id": "GQ_uIOJPGs38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, the data should be adjusted since it exhibited skewness.\n",
        "We applied logarithmic and square root transformations to various continuous columns in order to mitigate the data's skewed distribution."
      ],
      "metadata": {
        "id": "aFzlvkBMJqg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming 'continuous_variable' is a list of column names representing continuous variables in hd_df\n",
        "\n",
        "# Create an instance of the MinMaxScaler class\n",
        "scaler = MinMaxScaler()\n",
        "features = [i for i in hd_df.columns if i not in ['Presence_or_absence_of_cardiovascular_disease']]"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "w41uRGoyNnqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_variable"
      ],
      "metadata": {
        "id": "pfSrTKCtVhxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hd_df[continuous_variable] = scaler.fit_transform(hd_df[continuous_variable])"
      ],
      "metadata": {
        "id": "xXzK2_kHVhsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the x and y\n",
        "# Separating the target variable 'TenYearCHD' from the dataset and creating the feature matrix 'X'.\n",
        "y = hd_df['Presence_or_absence_of_cardiovascular_disease']  # Target variable: Whether an individual develops coronary heart disease in the next ten years.\n",
        "X = hd_df.drop(['Presence_or_absence_of_cardiovascular_disease'], axis=1)  # Feature matrix: Excluding 'Presence_or_absence_of_cardiovascular_disease' and 'id' columns for model training."
      ],
      "metadata": {
        "id": "DULBatgjVhlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Dimensionality reduction is unnecessary as we have already reduced the quantity of features, retaining only the essential ones."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=3697,stratify=y,shuffle=True)\n",
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "For model training, we divided our dataset into two parts: the training set and the testing set. This division was achieved using the \"train_test_split\" technique. Our data was divided in such a way that 80% was allocated for training, and the remaining 20% was reserved for testing. This distribution strikes a suitable balance between providing enough data for effective model training and having a sufficient amount of data for evaluating the model's performance on data it hasn't seen before. Allocating 80% for training ensures the model learns from a substantial dataset, and the remaining 20% serves as a means to gauge how well the model can generalize its learnings to new, unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Yes, the dataset is imbalanced and the number of positive cases is very low compared to the negative cases."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Assuming you have already imported y_train and it's a pandas Series\n",
        "value_counts = y_train.value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "ax = value_counts.plot(kind='bar', title='Target variable before SMOTE')\n",
        "\n",
        "# Annotate each bar with its count\n",
        "for i, v in enumerate(value_counts):\n",
        "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
        "\n",
        "\n",
        "# Improve visibility of x-axis labels\n",
        "ax.set_xticklabels(['No Cardiovascular Disease', 'Cardiovascular Disease'], fontsize=12,rotation=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversapling using SMOTETomek\n",
        "# fit predictor and and target variable\n",
        "x_smote,y_smote = SMOTETomek(random_state=0).fit_resample(X_train,y_train)\n",
        "\n",
        "print('Sample in the original dataset:' , len(y_train))\n",
        "print('Sample in the resampled dataset:' , len(y_smote))"
      ],
      "metadata": {
        "id": "tYTCN5vn4X1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_smote is a pandas Series or DataFrame\n",
        "value_counts = y_smote.value_counts()\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Create a bar plot with customized properties\n",
        "ax = value_counts.plot(kind='bar', color='skyblue', width=0.6, figsize=(8, 6))\n",
        "\n",
        "# Annotate each bar with its count and adjust text properties\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "# Add X and Y axis labels\n",
        "plt.xlabel('Target Classes', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "# Improve visibility of x-axis labels and set rotation to 0 degrees\n",
        "ax.set_xticklabels(['No Cardiovascular Disease', 'Cardiovascular Disease'], fontsize=12, rotation=0)\n",
        "\n",
        "# Add a title\n",
        "plt.title('Distribution of Target Variable after SMOTE', fontsize=16)\n",
        "\n",
        "# Add grid lines for reference\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bC8uMeaixEkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Addressing class imbalance involves increasing the representation of minority class samples through oversampling with SMOTE. Afterward, TOMEK links are eliminated to refine the balance. The process concludes with an assessment of class distribution before and after these steps to ensure balanced representation.**\n",
        "\n"
      ],
      "metadata": {
        "id": "TJBy5v2UAkC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Here we will be experimenting with 3 algorithms**\n",
        "## 1. Logistic regression.\n",
        "\n",
        "## 2. Decision Tree.\n",
        "\n",
        "## 3. Random Forest.\n",
        "\n",
        "## 4. SVM (Support Vector Machine).\n",
        "\n",
        "## 5. Xtreme Gradient Boosting.\n",
        "\n",
        "## 6. Naive Bayes.\n",
        "\n",
        "##7. Neural Network.\n",
        "\n",
        "## 8. Selection of best model."
      ],
      "metadata": {
        "id": "PF1AWFC46FHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "def evaluate_classification_model(classification_model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a classification machine learning model using various metrics and visualizations.\n",
        "\n",
        "    Parameters:\n",
        "        classification_model (object): The classification model to be evaluated.\n",
        "        X_train, X_test (DataFrame): Feature datasets for training and testing.\n",
        "        y_train, y_test (Series): Target datasets for training and testing.\n",
        "        feature_names (list): List of feature names.\n",
        "\n",
        "    Returns:\n",
        "        model_metrics (list): List of evaluation metrics for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fit the classification model on the training data\n",
        "    classification_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the training and test data\n",
        "    y_pred_train = classification_model.predict(X_train)\n",
        "    y_pred_test = classification_model.predict(X_test)\n",
        "\n",
        "    # Predict probabilities for ROC curve\n",
        "    pred_prob_train = classification_model.predict_proba(X_train)[:, 1]\n",
        "    pred_prob_test = classification_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate ROC AUC score for training and test sets\n",
        "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
        "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
        "    print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
        "    print(\"Test ROC AUC:\", roc_auc_test)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
        "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
        "    plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
        "    plt.legend()\n",
        "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate confusion matrix for training and test sets\n",
        "    confusion_matrix_train = confusion_matrix(y_train, y_pred_train)\n",
        "    confusion_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "\n",
        "    # Plot confusion matrix for training set\n",
        "    sns.heatmap(confusion_matrix_train, annot=True, fmt='d', cmap=\"Blues\", ax=ax[0])\n",
        "    ax[0].set_xlabel(\"Predicted Label\")\n",
        "    ax[0].set_ylabel(\"True Label\")\n",
        "    ax[0].set_title(\"Train Confusion Matrix\")\n",
        "\n",
        "    # Plot confusion matrix for test set\n",
        "    sns.heatmap(confusion_matrix_test, annot=True, fmt='d', cmap=\"Blues\", ax=ax[1])\n",
        "    ax[1].set_xlabel(\"Predicted Label\")\n",
        "    ax[1].set_ylabel(\"True Label\")\n",
        "    ax[1].set_title(\"Test Confusion Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate classification report for training and test sets\n",
        "    classification_report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
        "    classification_report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
        "    print(\"\\nTrain Classification Report:\")\n",
        "    print(pd.DataFrame(classification_report_train).transpose())\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    print(pd.DataFrame(classification_report_test).transpose())\n",
        "\n",
        "    # Convert the nested dictionary to a flat DataFrame for both training and test reports\n",
        "    df_train = pd.DataFrame(classification_report_train).transpose()\n",
        "    df_test = pd.DataFrame(classification_report_test).transpose()\n",
        "\n",
        "    # Create a subplot grid\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # Plot heatmap for training set classification report\n",
        "    sns.heatmap(data=df_train, annot=True, cmap='coolwarm', fmt=\".2f\", ax=axes[0])\n",
        "    axes[0].set_title(\"Train Classification Report\")\n",
        "    axes[0].set_xlabel(\"Metrics\")\n",
        "    axes[0].set_ylabel(\"Classes\")\n",
        "\n",
        "    # Plot heatmap for test set classification report\n",
        "    sns.heatmap(data=df_test, annot=True, cmap='coolwarm', fmt=\".2f\", ax=axes[1])\n",
        "    axes[1].set_title(\"Test Classification Report\")\n",
        "    axes[1].set_xlabel(\"Metrics\")\n",
        "    axes[1].set_ylabel(\"Classes\")\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the heatmaps\n",
        "    plt.show()\n",
        "\n",
        "    # Check if the classification model has feature importances attribute\n",
        "    if hasattr(classification_model, 'feature_importances_'):\n",
        "        # Get feature importances\n",
        "        feature_importance = classification_model.feature_importances_\n",
        "        # Create a Series for feature importances and sort it\n",
        "        feature_importance_series = pd.Series(feature_importance, index=features)\n",
        "        feature_importance_series = feature_importance_series.sort_values(ascending=False)\n",
        "        # Plot feature importances\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        feature_importance_series[:15].plot(kind='barh', color='skyblue')\n",
        "        plt.title('Top 15 Feature Importances')\n",
        "        plt.xlabel('Relative Importance')\n",
        "        plt.ylabel('Features')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nThe classification model does not have a feature importances attribute.\")\n",
        "\n",
        "    # Calculate additional evaluation metrics\n",
        "    precision_train = classification_report_train['1']['precision']\n",
        "    precision_test = classification_report_test['1']['precision']\n",
        "\n",
        "    recall_train = classification_report_train['1']['recall']\n",
        "    recall_test = classification_report_test['1']['recall']\n",
        "\n",
        "    accuracy_train = accuracy_score(y_true=y_train, y_pred=y_pred_train)\n",
        "    accuracy_test = accuracy_score(y_true=y_test, y_pred=y_pred_test)\n",
        "\n",
        "    f1_score_train = classification_report_train['1']['f1-score']\n",
        "    f1_score_test = classification_report_test['1']['f1-score']\n",
        "\n",
        "    # Store the evaluation metrics in a list\n",
        "    model_metrics = [precision_train, precision_test, recall_train, recall_test, accuracy_train, accuracy_test, roc_auc_train, roc_auc_test, f1_score_train, f1_score_test]\n",
        "\n",
        "    # Return the list of evaluation metrics\n",
        "    return model_metrics\n"
      ],
      "metadata": {
        "id": "VakmUWMl5pCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **First ML Model**\n",
        "\n",
        "## **Logistic Regression**"
      ],
      "metadata": {
        "id": "n3XTZvK6a5I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to drop from the DataFrame for correlation analysis\n",
        "columns_to_drop = ['id',]\n",
        "\n",
        "# Create a new DataFrame by dropping specified columns\n",
        "filtered_df = data_set.drop(columns=columns_to_drop)\n",
        "\n",
        "# Calculate the correlation matrix for the filtered DataFrame\n",
        "correlation_matrix = filtered_df.corr()\n",
        "\n",
        "# Get the index of top correlated features from the correlation matrix\n",
        "top_corr_features = correlation_matrix.index\n",
        "\n",
        "# Set the size of the heatmap figure\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Plot a heatmap of correlations between the top features\n",
        "# Use Seaborn's heatmap function ('sns.heatmap') with annotations and a specific color map ('RdYlGn')\n",
        "heatmap = sns.heatmap(filtered_df[top_corr_features].corr(), annot=True, cmap='RdYlGn')\n",
        "\n",
        "# Set the title and labels for the heatmap\n",
        "heatmap.set_title('Correlation Heatmap', fontsize=18)\n",
        "plt.xlabel('Features', fontsize=14)\n",
        "plt.ylabel('Features', fontsize=14)\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bMgRSYti2TXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_varnc_inflasn_factr(hd_df[[c for c in cont_var_df]])# Calculate VIF for the selected continuous variables using the defined function"
      ],
      "metadata": {
        "id": "rlnevU3LvY0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix for the specified continuous variable in the DataFrame (hd_df)\n",
        "corr = hd_df[continuous_variable].corr()\n",
        "\n",
        "# Set the size of the heatmap figure for better visualization\n",
        "plt.figure(figsize=(25, 10))\n",
        "\n",
        "# Plot a heatmap of correlations for the continuous variable\n",
        "# Use Seaborn's heatmap function ('sns.heatmap') with annotations ('annot=True') and a specific color map ('cmap=plt.cm.Accent_r')\n",
        "sns.heatmap(corr, annot=True, cmap=plt.cm.Accent_r)\n",
        "\n",
        "# Add annotations to the heatmap, showing correlation values in each cell for better insights\n",
        "# Correlation values closer to 1 indicate a strong positive correlation, while values closer to -1 indicate a strong negative correlation\n",
        "# The color intensity represents the strength of the correlation, with lighter colors indicating stronger correlations\n",
        "\n",
        "# Display the heatmap visualization to explore the correlation relationships among the variables\n"
      ],
      "metadata": {
        "id": "MFwgZwGPvmjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Logistic Regression classifier with specified parameters\n",
        "classification_model = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "# Call the custom function 'evaluate_classification_model' to evaluate the Logistic Regression model\n",
        "# Pass the classifier, SMOTE transformed training features (x_smote), original test features (X_test),\n",
        "# SMOTE transformed training labels (y_smote), and original test labels (y_test)\n",
        "# Returns a list of classification evaluation metrics\n",
        "LR_classification_metrics = evaluate_classification_model(classification_model, x_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "_5AploTYYH76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "Cp6ZXPcYiY7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [100,10,1,0.1,0.01,0.001,0.0001],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Initializing the logistic regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, max_iter=10000, random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=4, random_state=0)\n",
        "\n",
        "# Using GridSearchCV to tune the hyperparameters using cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=rskf)\n",
        "grid.fit(x_smote, y_smote)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "# The best hyperparameters found by GridSearchCV\n",
        "print(\"Best hyperparameters: \", best_params)"
      ],
      "metadata": {
        "id": "zQsdVrygYH2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "LR_classification_metrics1 = LogisticRegression(C=best_params['C'],\n",
        "                                  penalty=best_params['penalty'],\n",
        "                                  solver=best_params['solver'],\n",
        "                                  max_iter=10000, random_state=0)"
      ],
      "metadata": {
        "id": "7P_1r7zUYHyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "LR_classification_metrics2 = evaluate_classification_model(LR_classification_metrics1, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "9_ya75aUYHv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which hyperparameter optimization technique have you used and why?**"
      ],
      "metadata": {
        "id": "4ANTOmlTmAei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method employed for optimizing hyperparameters is GridSearchCV. GridSearchCV involves a thorough exploration across a defined set of parameters to identify the most suitable ones for a model. It's widely used due to its simplicity and effectiveness in finding optimal hyperparameters.\n",
        "\n",
        "Selecting an appropriate hyperparameter optimization method depends on factors like the complexity of the parameter options, available computational power, and time limitations. GridSearchCV is particularly useful when the parameter space isn't overly large, and computational resources are reasonably abundant."
      ],
      "metadata": {
        "id": "iFDqU8ormEHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "B49Ryya9oLlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a score dataframe\n",
        "score_classification = pd.DataFrame(index = ['Precision Train', 'Precision Test','Recall Train','Recall Test','Accuracy Train', 'Accuracy Test','ROC-AUC Train', 'ROC-AUC Test','F1 macro Train', 'F1 macro Test'])\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score = LR_classification_metrics\n",
        "\n",
        "score_classification['Logistic regression'] = lr_score\n",
        "\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score2 = LR_classification_metrics2\n",
        "\n",
        "score_classification['Logistic regression tuned'] = lr_score2"
      ],
      "metadata": {
        "id": "FU-xYRklqe5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification"
      ],
      "metadata": {
        "id": "gBU3Ath9YHs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that adjusting the hyperparameters did not enhance the Logistic Regression model's performance on the test data. The precision, recall, accuracy, ROC-AUC, and F1 scores remain unchanged between the original and optimized Logistic Regression models when evaluated on the test set."
      ],
      "metadata": {
        "id": "2mzTLDqhC770"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Second ML Model**\n",
        "## **Decision Tree**\n"
      ],
      "metadata": {
        "id": "8AkbMMEfFoRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "1tMhkc8IX0L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "dt_classification = DecisionTreeClassifier(random_state=20)"
      ],
      "metadata": {
        "id": "KG_7WLcrYHqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "dt_classification_score = evaluate_classification_model(dt_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "EKg22sOsYHnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "TMNx5kiuYiuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "grid = {'max_depth' : [3,4,5,6,7,8],\n",
        "        'min_samples_split' : np.arange(2,8),\n",
        "        'min_samples_leaf' : np.arange(10,20)}\n",
        "\n",
        "# Initialize the model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, grid, cv=rskf)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(x_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "nPzfodA3YdaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a new model with the best hyperparameters\n",
        "dt2_classification = DecisionTreeClassifier(max_depth=best_params['max_depth'],\n",
        "                                 min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                                 min_samples_split=best_params['min_samples_split'],\n",
        "                                 random_state=20)"
      ],
      "metadata": {
        "id": "4N02wZ2xYdOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt2_classification_score = evaluate_classification_model(dt2_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "54p2NrH0Yztz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification['Decision Tree'] = dt_classification_score\n",
        "score_classification['Decision Tree tuned'] = dt2_classification_score"
      ],
      "metadata": {
        "id": "o8NtUqfbYHkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Which hyperparameter optimization technique have you used and why?***\n",
        "\n",
        "\n",
        "The approach employed for hyperparameter optimization is GridSearchCV, a method that thoroughly explores a predefined set of parameters to identify the optimal ones for a model. GridSearchCV stands out for its simplicity and effectiveness in finding the best hyperparameters.\n",
        "\n",
        "Selecting a hyperparameter optimization technique involves considering factors like the parameter space's complexity, available computational resources, and time constraints. GridSearchCV is suitable when the parameter space is manageable in size, and computational resources are not severely limited, making it a practical choice under these conditions."
      ],
      "metadata": {
        "id": "nxt1apwBaCi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.***"
      ],
      "metadata": {
        "id": "mF-muRIdaki2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification"
      ],
      "metadata": {
        "id": "LQlYuzOxYzRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning seems to have enhanced the Decision Tree model's performance on the test set. The optimized Decision Tree model demonstrates improved precision and F1 score compared to the original model. However, the recall, accuracy, and ROC-AUC scores experienced marginal declines after tuning.\n",
        "\n",
        "Unlike the original model, the tuned version doesn't exhibit overfitting."
      ],
      "metadata": {
        "id": "JKOt_ZA-brnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Third ML Model***\n",
        "## ***Random Forest***\n"
      ],
      "metadata": {
        "id": "soAzJen7bx1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "rf_classification = RandomForestClassifier(random_state=0)"
      ],
      "metadata": {
        "id": "nx-eJkLTYzVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.***"
      ],
      "metadata": {
        "id": "Vg4iIdm8dfJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf_classification_score = evaluate_classification_model(rf_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "ZcdWiRlPYzaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification['Random Forest'] = rf_classification_score"
      ],
      "metadata": {
        "id": "lF74XvuBYHha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***2. Cross- Validation & Hyperparameter Tuning***"
      ],
      "metadata": {
        "id": "sVhhEYe8d4N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "grid = {'n_estimators': [10, 50, 100, 200],\n",
        "              'max_depth': [8, 9, 10, 11, 12,13, 14, 15],\n",
        "              'min_samples_split': [2, 3, 4, 5]}\n",
        "\n",
        "# Initialize the model\n",
        "rf_classification = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf_classification = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomSearchCV\n",
        "random_classification_search = RandomizedSearchCV(rf_classification, grid,cv=rskf, n_iter=10, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "random_classification_search.fit(x_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_classification_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "Dn-3uHs1YHeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "rf2_classification = RandomForestClassifier(n_estimators = best_params['n_estimators'],\n",
        "                                 min_samples_leaf= best_params['min_samples_split'],\n",
        "                                 max_depth = best_params['max_depth'],\n",
        "                                 random_state=0)"
      ],
      "metadata": {
        "id": "VZQK2OysYHbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score_classification chart\n",
        "rf2_classification_score = evaluate_classification_model(rf2_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "j19aC71vYHXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Which hyperparameter optimization technique have you used and why?**\n",
        "\n",
        "The hyperparameter optimization technique used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited."
      ],
      "metadata": {
        "id": "M-h_KA3heXqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "13TWerG-evCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification['Random Forest'] = rf_classification_score\n",
        "score_classification['Random Forest tuned'] = rf2_classification_score\n",
        "score_classification"
      ],
      "metadata": {
        "id": "JToI4TF7YHPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest model's performance on the test set significantly enhanced after hyperparameter tuning. The tuned model exhibited superior precision, recall, accuracy, and F1 score in comparison to the original, untuned version. Additionally, the ROC-AUC score on the test set also saw a slight improvement after the tuning process."
      ],
      "metadata": {
        "id": "I_wN20k5f8xG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fourth ML Model**  \n",
        "## **SVM (Support Vector Machine)**\n"
      ],
      "metadata": {
        "id": "VigtZHVOgFOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "svm_classification = SVC(kernel='linear', random_state=0, probability=True)"
      ],
      "metadata": {
        "id": "ouQxZPJiYHMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "kgnqsZmzg6RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score_classification chart\n",
        "svm_classification_score = evaluate_classification_model(svm_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "d_UP3a66YHJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "GvuT4DPahWeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': np.arange(0.1, 10, 0.1),\n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              'degree': np.arange(2, 6, 1)}\n",
        "\n",
        "# Initialize the model\n",
        "svm2_classification = SVC(random_state=0, probability=True)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf_classification = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV with 6-fold cross-validation\n",
        "random_classification_search = RandomizedSearchCV(svm2_classification, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_classification_search.fit(x_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_classification_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "0b56NPavYHDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "svm2_classification = SVC(C = best_params['C'],\n",
        "           kernel = best_params['kernel'],\n",
        "           degree = best_params['degree'],\n",
        "           random_state=0, probability=True)"
      ],
      "metadata": {
        "id": "OHp0P-e6hi_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the SVM classification model using the provided function.\n",
        "# The function evaluates the model's performance on the SMOTE oversampled training data (x_smote, y_smote)\n",
        "# and the unseen test data (X_test, y_test).\n",
        "# It calculates various classification metrics such as precision, recall, accuracy, ROC AUC, and F1-score.\n",
        "# It also generates visualizations including ROC curves, confusion matrices, and feature importances (if applicable).\n",
        "\n",
        "# Call the evaluate_classification_model function with the SVM classification model (svm2_classification)\n",
        "# and the training and test datasets (x_smote, X_test, y_smote, y_test) along with feature names if needed.\n",
        "svm2_classification_score = evaluate_classification_model(svm2_classification, x_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "ikti8U94hm9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Which hyperparameter optimization technique have you used and why?**"
      ],
      "metadata": {
        "id": "KMKh_iwOh6Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this approach, Randomized Search is employed. This method is favored because it's often more efficient than exhaustive techniques like grid search. Rather than exploring every conceivable combination of hyperparameters, randomized search randomly selects a subset of the hyperparameter space. This strategy conserves time and computational power while still uncovering effective hyperparameters for the model."
      ],
      "metadata": {
        "id": "AJBWtuc9iK4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "6feoCPJWiQWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the classification scores for SVM model to the 'score_classification' DataFrame.\n",
        "score_classification['SVM'] = svm_classification_score\n",
        "\n",
        "# Add the classification scores for tuned SVM model to the 'score_classification' DataFrame.\n",
        "score_classification['SVM tuned'] = svm2_classification_score\n",
        "\n",
        "# Display the updated 'score_classification' DataFrame showing scores for both SVM models.\n",
        "score_classification\n"
      ],
      "metadata": {
        "id": "nbPw5Iw_YHGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimized SVM model, resulting from hyperparameter tuning, demonstrates enhanced performance on the test set. It exhibits improved recall, accuracy, and F1 score in comparison to the original, non-optimized SVM model. Nevertheless, precision and ROC-AUC scores experienced a minor decrease after tuning."
      ],
      "metadata": {
        "id": "QEZNXxFTir_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 5**\n",
        "### **Xtreme Gradient Boosting**\n"
      ],
      "metadata": {
        "id": "LsAbvsycj9VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Initialize the model\n",
        "xgb_classification_model = xgb.XGBClassifier()\n"
      ],
      "metadata": {
        "id": "EMvoFfgbQfRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "4kwCEpVNkSQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "xgb_classification_score = evaluate_classification_model(xgb_classification_model, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "gkrgQOhBhm2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "dt4rmP6mkrbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {'learning_rate': np.arange(0.01, 0.3, 0.01),\n",
        "              'max_depth': np.arange(3, 15, 1),\n",
        "              'n_estimators': np.arange(100, 200, 10)}\n",
        "\n",
        "# Initialize the model\n",
        "xgb2_model = xgb.XGBClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(xgb2_model, param_grid, n_iter=10, cv=rskf)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(x_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "6l44PuqkkwF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "xgb2_classification_model = xgb.XGBClassifier(learning_rate = best_params['learning_rate'],\n",
        "                                 max_depth = best_params['max_depth'],\n",
        "                               n_estimators = best_params['n_estimators'],\n",
        "                                 random_state=0)"
      ],
      "metadata": {
        "id": "CJPBJQ7Ik6d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "xgb2_classification_score = evaluate_classification_model(xgb2_classification_model, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "DVq-dUDbk9wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Which hyperparameter optimization technique have you used and why?**\n",
        "\n",
        "\n",
        "In this context, we've utilized Randomized Search to optimize the XGB model.\n",
        "\n",
        "Randomized Search is favored due to its efficiency compared to exhaustive methods like grid search. Rather than exploring every conceivable combination of hyperparameters, randomized search selects a random subset from the hyperparameter space. This approach conserves time and computational power while still identifying effective hyperparameters for the model.\n",
        "\n",
        "\n",
        "### **Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "f6ufmH9MlGfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the 'xgb_classification_score' list to the 'score_classification' DataFrame with the key 'XGB'\n",
        "score_classification['XGB'] = xgb_classification_score\n",
        "\n",
        "# Adding the 'xgb2_classification_score' list to the 'score_classification' DataFrame with the key 'XGB tuned'\n",
        "score_classification['XGB tuned'] = xgb2_classification_score\n",
        "\n",
        "# Displaying the updated 'score_classification' DataFrame, now containing scores for both 'XGB' and 'XGB tuned' models\n",
        "score_classification\n"
      ],
      "metadata": {
        "id": "mpzMHHxRhmk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the XGBoost model on the test set. The tuned XGBoost model has higher precision, recall, accuracy, and F1 score on the test set compared to the untuned XGBoost model. The ROC-AUC score on the test set also improved slightly after tuning."
      ],
      "metadata": {
        "id": "eONR7j0OlotX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sixth ML Model**\n",
        "## **Naive Bayes**\n"
      ],
      "metadata": {
        "id": "WBSZS3illtB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initiate model\n",
        "naive_classification = GaussianNB()\n"
      ],
      "metadata": {
        "id": "phqLYqMlYHAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "Fesc4rOCmFGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "naive_classification_score = evaluate_classification_model(naive_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "tHQ5Bh51YG9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "q227UE9lmV_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "# Initialize the model\n",
        "naive = GaussianNB()\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=4, n_repeats=4, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = GridSearchCV(naive, param_grid, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(x_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "leOguHOHmZ7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "naive2_classification = GaussianNB(var_smoothing = best_params['var_smoothing'])"
      ],
      "metadata": {
        "id": "VxtowUgWmeMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "naive2_classification_score = evaluate_classification_model(naive2_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "SxE25IUimeGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Which hyperparameter optimization technique have you used and why?**\n",
        "\n",
        "Here we have used the gridsearch for optimization of the Naive Bayes model.\n",
        "\n",
        "Grid search is an exhaustive search method that tries all possible combinations of hyperparameters specified in the hyperparameter grid. This technique can be useful when the number of hyperparameters to tune is small and the range of possible values for each hyperparameter is limited. Grid search can find the best combination of hyperparameters, but it can be computationally expensive for large hyperparameter grids.\n",
        "\n",
        "### **Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "rh96bM4dnXZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the 'Naive Bayes' column in the 'score_classification' DataFrame\n",
        "# with the evaluation metrics obtained from the 'naive_classification_score'\n",
        "score_classification['Naive Bayes'] = naive_classification_score\n",
        "\n",
        "# Update the 'Naive Bayes tuned' column in the 'score_classification' DataFrame\n",
        "# with the evaluation metrics obtained from the 'naive2_classification_score'\n",
        "score_classification['Naive Bayes tuned'] = naive2_classification_score\n",
        "\n",
        "# Display the updated 'score_classification' DataFrame with the added columns\n",
        "score_classification\n"
      ],
      "metadata": {
        "id": "I-Egq1HUYG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Seventh ML Model**\n",
        "## **Neural Network**\n"
      ],
      "metadata": {
        "id": "cEyjKyy5n1-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initialize the MLPClassifier\n",
        "neural_classification = MLPClassifier(random_state=0)\n"
      ],
      "metadata": {
        "id": "TijsnhQWYG3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "9_9lGnVLoIev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural_classification_score = evaluate_classification_model(neural_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "qraTJuFSYG0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "7NkItoqvoZWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {'hidden_layer_sizes': np.arange(10, 100, 10),\n",
        "              'alpha': np.arange(0.0001, 0.01, 0.0001)}\n",
        "# Initialize the model\n",
        "neural = MLPClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(neural, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(x_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "kA-MpnkcodLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "neural2_classification = MLPClassifier(hidden_layer_sizes = best_params['hidden_layer_sizes'],\n",
        "                        alpha = best_params['alpha'],\n",
        "                        random_state = 0)"
      ],
      "metadata": {
        "id": "LAKE0B5Uompo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural2_classification_score = evaluate_classification_model(neural2_classification, x_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "5cthrZo8oqty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Which hyperparameter optimization technique have you used and why?**\n",
        "Here we have used Randomized search to tune the Neural Network model.\n",
        "\n",
        "Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model.\n",
        "\n",
        "### **Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "mbPUnIjTo03a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the evaluation scores for the 'Neural Network' model to the 'score_classification' DataFrame\n",
        "score_classification['Neural Network'] = neural_classification_score\n",
        "\n",
        "# Adding the evaluation scores for the tuned 'Neural Network' model to the 'score_classification' DataFrame\n",
        "score_classification['Neural Network tuned'] = neural2_classification_score\n",
        "\n",
        "# Displaying the updated 'score_classification' DataFrame, now including scores for both the 'Neural Network' and 'Neural Network tuned' models\n",
        "score_classification\n"
      ],
      "metadata": {
        "id": "DK_YImgXYGxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot of scores for models**\n",
        "\n",
        "## **Precision**"
      ],
      "metadata": {
        "id": "tkm1H-lMpJ6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data for visualization\n",
        "models = list(score_classification.columns)\n",
        "train_precision = score_classification.iloc[0, :]\n",
        "test_precision = score_classification.iloc[1, :]\n",
        "\n",
        "# Setting the positions for bars on X-axis\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "# Setting figure size and creating a bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars1 = plt.bar(X_axis - 0.2, train_precision, 0.4, label='Train Precision', color='b', alpha=0.7)\n",
        "bars2 = plt.bar(X_axis + 0.2, test_precision, 0.4, label='Test Precision', color='g', alpha=0.7)\n",
        "\n",
        "# Setting X-axis labels, rotating them for better visibility\n",
        "plt.xticks(X_axis, models, rotation=30, ha='right')\n",
        "\n",
        "# Setting Y-axis label and plot title\n",
        "plt.ylabel(\"Precision Score\")\n",
        "plt.title(\"Model Comparison: Train vs. Test Precision\")\n",
        "\n",
        "# Adding annotations to the bars\n",
        "for bar in bars1:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom', color='black', fontsize=8)\n",
        "\n",
        "for bar in bars2:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom', color='black', fontsize=8)\n",
        "\n",
        "# Adding legend to the plot\n",
        "plt.legend()\n",
        "\n",
        "# Displaying the plot\n",
        "plt.tight_layout()  # Ensures the labels fit well in the plot area\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WXuI_wybDguG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models, train precision scores, and test precision scores\n",
        "models = list(score_classification.columns)\n",
        "train_precision = score_classification.iloc[0, :]\n",
        "test_precision = score_classification.iloc[1, :]\n",
        "\n",
        "# Set the positions for the bars on the X-axis\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot bars for train and test precision scores\n",
        "plt.bar(X_axis - 0.2, train_precision, 0.4, label='Train Precision', color='skyblue', edgecolor='grey', linewidth=0.5)\n",
        "plt.bar(X_axis + 0.2, test_precision, 0.4, label='Test Precision', color='orange', edgecolor='grey', linewidth=0.5)\n",
        "\n",
        "# Annotate the bars with their respective values\n",
        "for i in range(len(models)):\n",
        "    plt.text(X_axis[i] - 0.2, train_precision[i] + 0.01, str(round(train_precision[i], 2)), ha='center')\n",
        "    plt.text(X_axis[i] + 0.2, test_precision[i] + 0.01, str(round(test_precision[i], 2)), ha='center')\n",
        "\n",
        "# Set the X-axis labels and rotate them for better visibility\n",
        "plt.xticks(X_axis, models, rotation=45, ha='right')\n",
        "\n",
        "# Set labels and title\n",
        "plt.ylabel(\"Precision Score\")\n",
        "plt.title(\"Precision Scores for Different Models\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3dyrY06ID_Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall Scores plot\n",
        "\n",
        "# Define the models, train precision scores, and test precision scores\n",
        "models = list(score_classification.columns)\n",
        "train_precision = score_classification.iloc[0, :]\n",
        "test_precision = score_classification.iloc[1, :]\n",
        "\n",
        "# Set the positions for the bars on the X-axis\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot bars for train and test precision scores\n",
        "plt.bar(X_axis - 0.2, train_precision, 0.4, label='Train Precision', color='skyblue', edgecolor='grey', linewidth=0.5)\n",
        "plt.bar(X_axis + 0.2, test_precision, 0.4, label='Test Precision', color='orange', edgecolor='grey', linewidth=0.5)\n",
        "\n",
        "# Annotate the bars with their respective values\n",
        "for i in range(len(models)):\n",
        "    plt.text(X_axis[i] - 0.2, train_precision[i] + 0.01, str(round(train_precision[i], 2)), ha='center')\n",
        "    plt.text(X_axis[i] + 0.2, test_precision[i] + 0.01, str(round(test_precision[i], 2)), ha='center')\n",
        "\n",
        "# Set the X-axis labels and rotate them for better visibility\n",
        "plt.xticks(X_axis, models, rotation=45, ha='right')\n",
        "\n",
        "# Set labels and title\n",
        "plt.ylabel(\"Precision Score\")\n",
        "plt.title(\"Precision Scores for Different Models\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UWRz8ndZYGuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recall**"
      ],
      "metadata": {
        "id": "eCMrhULdqyTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall Scores plot\n",
        "\n",
        "# Data\n",
        "models = list(score_classification.columns)\n",
        "train_recall = score_classification.iloc[2, :]  # Assuming score contains relevant data, adjust index if needed\n",
        "test_recall = score_classification.iloc[3, :]   # Assuming score contains relevant data, adjust index if needed\n",
        "\n",
        "# Set the positions and width for the bars\n",
        "X_axis = np.arange(len(models))\n",
        "bar_width = 0.4\n",
        "\n",
        "# Create a figure and axis for the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting train recall scores\n",
        "plt.bar(X_axis - bar_width/2, train_recall, bar_width, label='Train Recall', color='skyblue', edgecolor='black')\n",
        "\n",
        "# Plotting test recall scores\n",
        "plt.bar(X_axis + bar_width/2, test_recall, bar_width, label='Test Recall', color='orange', edgecolor='black')\n",
        "\n",
        "# Set x-axis labels and rotate them for better visibility\n",
        "plt.xticks(X_axis, models, rotation=45, ha='right')\n",
        "\n",
        "# Adding data values as annotations above the bars\n",
        "for i in range(len(models)):\n",
        "    plt.text(X_axis[i] - 0.2, train_recall[i] + 0.01, str(round(train_recall[i], 2)), ha='center', color='b')\n",
        "    plt.text(X_axis[i] + 0.2, test_recall[i] + 0.01, str(round(test_recall[i], 2)), ha='center', color='orange')\n",
        "\n",
        "# Set y-axis label and plot title\n",
        "plt.ylabel(\"Recall Score\")\n",
        "plt.title(\"Comparison of Train and Test Recall Scores for Different Models\")\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "viofTzX1YGrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Accuracy**"
      ],
      "metadata": {
        "id": "RwImxOCOq_7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data for visualization (assuming 'score' is a DataFrame with accuracy scores)\n",
        "models = list(score_classification.columns)\n",
        "train = score_classification.iloc[4, :]\n",
        "test = score_classification.iloc[5, :]\n",
        "\n",
        "# Set the positions for the bars on X-axis\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "# Set the size of the figure\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting the bars for train and test accuracy\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label='Train Accuracy', color='b', alpha=0.7)\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label='Test Accuracy', color='g', alpha=0.7)\n",
        "\n",
        "# Annotate the bars with accuracy values\n",
        "for i in range(len(models)):\n",
        "    plt.text(X_axis[i] - 0.2, train[i] + 0.01, f'{train[i]:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "    plt.text(X_axis[i] + 0.2, test[i] + 0.01, f'{test[i]:.2f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Set X-axis ticks and labels (model names) with rotation for better readability\n",
        "plt.xticks(X_axis, models, rotation=30)\n",
        "\n",
        "# Set Y-axis label and plot title\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.title(\"Accuracy Score Comparison for Different Models\")\n",
        "\n",
        "# Add legend to the plot\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_xxHrV73GTbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ROC-AUC**"
      ],
      "metadata": {
        "id": "IAV9lBu_rMRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models, train, and test data\n",
        "models = list(score_classification.columns)\n",
        "train_scores = score_classification.iloc[6, :]\n",
        "test_scores = score_classification.iloc[7, :]\n",
        "\n",
        "# Set the width of the bars and the positions for each model\n",
        "bar_width = 0.35\n",
        "r1 = np.arange(len(models))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot train ROC-AUC scores\n",
        "plt.bar(r1, train_scores, width=bar_width, edgecolor='grey', label='Train ROC-AUC', alpha=0.7)\n",
        "\n",
        "# Plot test ROC-AUC scores\n",
        "plt.bar(r2, test_scores, width=bar_width, edgecolor='grey', label='Test ROC-AUC', alpha=0.7)\n",
        "\n",
        "# Add data labels above the bars\n",
        "for i in range(len(models)):\n",
        "    plt.text(i, train_scores[i] + 0.01, f'{train_scores[i]:.2f}', ha='center')\n",
        "    plt.text(i + bar_width, test_scores[i] + 0.01, f'{test_scores[i]:.2f}', ha='center')\n",
        "\n",
        "# Customize the plot\n",
        "plt.xlabel('Models', fontweight='bold')\n",
        "plt.xticks([r + bar_width / 2 for r in range(len(models))], models, rotation=45, ha='right')\n",
        "plt.ylabel('ROC-AUC Score', fontweight='bold')\n",
        "plt.title('ROC-AUC Scores for Each Model', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3cxuNMnXIEAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **F1 score**"
      ],
      "metadata": {
        "id": "J8gyRPqjrhn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "models = list(score_classification.columns)\n",
        "train_f1_scores = score_classification.iloc[8, :]\n",
        "test_f1_scores = score_classification.iloc[9, :]\n",
        "x_ticks_labels = models\n",
        "\n",
        "# X-axis positions for the bars\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "# Width of the bars\n",
        "bar_width = 0.4\n",
        "\n",
        "# Create a figure and axis\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting the bars for train F1 scores\n",
        "plt.bar(X_axis - bar_width/2, train_f1_scores, bar_width, label='Train F1 macro', color='skyblue', alpha=0.7)\n",
        "\n",
        "# Plotting the bars for test F1 scores\n",
        "plt.bar(X_axis + bar_width/2, test_f1_scores, bar_width, label='Test F1 macro', color='orange', alpha=0.7)\n",
        "\n",
        "# Setting x-ticks and labels\n",
        "plt.xticks(X_axis, x_ticks_labels, rotation=45, ha='right')\n",
        "\n",
        "# Adding annotations to the bars\n",
        "for i, v in enumerate(train_f1_scores):\n",
        "    plt.text(i - bar_width/2 - 0.03, v + 0.01, str(round(v, 2)), fontsize=10, ha='center', va='bottom')\n",
        "\n",
        "for i, v in enumerate(test_f1_scores):\n",
        "    plt.text(i + bar_width/2 + 0.03, v + 0.01, str(round(v, 2)), fontsize=10, ha='center', va='bottom')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.ylabel(\"F1 macro Score\")\n",
        "plt.title(\"F1 macro scores for each model (Train vs Test)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YvnqQKeOIcbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selection of best model**"
      ],
      "metadata": {
        "id": "KdQcWzvdrrpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_classification"
      ],
      "metadata": {
        "id": "bNOtar5nYGfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the overfitted models which have recall, rocauc, f1 for train as 1\n",
        "score_t = score_classification.transpose()            #taking transpose of the score dataframe to create new difference column\n",
        "remove_models = score_t[score_t['Recall Train']>=0.95].index  #creating a list of models which have 1 for train and score_t['Accuracy Train']==1.0 and score_t['ROC-AUC Train']==1.0 and score_t['F1 macro Train']==1.0\n",
        "remove_models\n",
        "\n",
        "adj = score_t.drop(remove_models)                     #creating a new dataframe with required models\n",
        "adj"
      ],
      "metadata": {
        "id": "ar6mzQzyYGcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_model(df, metrics):\n",
        "\n",
        "    best_models = {}\n",
        "    for metric in metrics:\n",
        "        max_test = df[metric + ' Test'].max()\n",
        "        best_model_test = df[df[metric + ' Test'] == max_test].index[0]\n",
        "        best_model = best_model_test\n",
        "        best_models[metric] = best_model\n",
        "    return best_models"
      ],
      "metadata": {
        "id": "ZDywoilTYGZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['Precision','Recall', 'Accuracy', 'ROC-AUC', 'F1 macro']\n",
        "\n",
        "best_models = select_best_model(adj, metrics)\n",
        "print(\"The best models are:\")\n",
        "for metric, best_model in best_models.items():\n",
        "    print(f\"{metric}: {best_model} - {adj[metric+' Test'][best_model].round(4)}\")"
      ],
      "metadata": {
        "id": "snOsTFlPrU2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Which Evaluation metrics did you consider for a positive business impact and why?**\n",
        "After thoughtful evaluation of the implications of incorrect results, specifically both false positives and false negatives within the scope of our business goals, I have chosen \"recall\" as the key measurement for our CHD risk prediction model. This implies our focus is on maximizing accurate identification of individuals with CHD risk (true positives) while minimizing cases where patients with CHD risk are overlooked (false negatives). The objective is to identify the most CHD risk cases correctly, even if it results in a few incorrect positive identifications.\n",
        "\n",
        "### **2. Which ML model did you choose from the above created models as your final prediction model and why?**\n",
        "Having assessed various machine learning models using the Framingham Heart Study dataset, I've opted for the SVM as our ultimate predictive model. This choice was made after considering the model's performance using our main evaluation measure, recall. Recall gauges the model's accuracy in identifying patients at risk of CHD. Our analysis revealed that, among the models we tested, the SVM achieved the highest recall score.\n",
        "\n",
        "We opted for using recall as our main evaluation measure because accurately recognizing patients at risk of coronary heart disease (CHD) is vital for meeting our business goals. Prioritizing a model with a high recall score means our focus is on correctly identifying as many CHD risk patients as we can, even if it results in some false positives. In summary, we are confident that the SVM stands as the most suitable choice for us, ensuring a positive impact on our business objectives.\n",
        "\n",
        "### **3. Explain the model which you have used and the feature importance using any model explainability tool?**\n",
        "# **SHAP(Shapley additive Explanations)**"
      ],
      "metadata": {
        "id": "yPs--Z87rl9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap\n"
      ],
      "metadata": {
        "id": "e7SwOpvmrUK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "9QE0XZQSrUHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the background dataset using k-means clustering\n",
        "X_summary = shap.kmeans(X, 100)\n",
        "\n",
        "# create an explainer object\n",
        "explainer = shap.KernelExplainer(neural_classification.predict_proba, X_summary)\n",
        "\n",
        "# compute the SHAP values for all the samples in the test data\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "0u9o1wQXrTwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summery plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=features)"
      ],
      "metadata": {
        "id": "LRrVGUGnYGWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar chart illustrates significant features along with their average Shap values, representing their average influence on the model's output strength. However, it doesn't indicate whether this impact is positive or negative on the predictions."
      ],
      "metadata": {
        "id": "pLgllw-stIVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.** **Future Work (Optional)**\n",
        "## **1. Save the best performing ml model in a pickle file or joblib file format for deployment process.**"
      ],
      "metadata": {
        "id": "cSaOIyq1tl1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pickle\n",
        "import pickle\n",
        "\n",
        "# Save the best model (naive bayes tuned)\n",
        "pickle.dump(naive2_classification, open('neural2.pkl', 'wb'))\n",
        "# Save the scaler\n",
        "pickle.dump(scaler, open('scaler.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "AGpWNvzCtdOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Again Load the saved model file and try to predict unseen data for a sanity check.**"
      ],
      "metadata": {
        "id": "87CvcF_Wt-tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "pickled_model = pickle.load(open('neural2.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "lTYzJdmptfem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance = X_test.loc[54]"
      ],
      "metadata": {
        "id": "Mux8cQfntfau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance"
      ],
      "metadata": {
        "id": "Dx9POx4Ate8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an array for the x test value for the 50 index row\n",
        "predict_new = np.array(instance).reshape(1,-1)\n",
        "\n",
        "# Testing on one instance which we used for shap X_test[50,:]\n",
        "pickled_model.predict(predict_new)"
      ],
      "metadata": {
        "id": "6TZPSYVmte4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The model has been successfully built and is prepared for deployment on a live server, where it can interact with real users.**\n",
        "\n",
        "\n",
        "\n",
        "# **Conclusion**\n",
        "In summary, this project showcased the effectiveness of machine learning methods in accurately foreseeing the 10-year risk of coronary heart disease (CHD) in patients. The study utilized data from an ongoing cardiovascular research initiative. Key findings from this endeavor encompass the following:\n",
        "\n",
        "1. **Refined data preprocessing and transformation significantly enhanced the performance of machine learning models, leading to more precise predictions.**\n",
        "\n",
        "2. **Careful selection of features played a vital role in identifying the most pertinent indicators of CHD risk.**\n",
        "\n",
        "3. **To ensure we catch all instances of heart disease in patients, we need a high Recall value. On the other hand, if we want to minimize the chances of mistakenly diagnosing a patient without heart disease, a high Precision value is necessary.**\n",
        "\n",
        "4. **Suppose we consider patients who were wrongly identified as having heart disease. In our context, these cases are crucial because they might indicate other health issues. Therefore, we aim to strike a balance between Precision and Recall metrics. Achieving a high F1 score is our goal, emphasizing the importance of correctly identifying these cases while minimizing false positives and false negatives.**\n",
        "\n",
        "5. **The disparity in performance between the training and test sets can be attributed to the introduction of synthetic data points to address the significant class imbalance in the training set. This discrepancy arises from the dissimilarity in data distribution between the training and test sets. Therefore, the impressive model performance on the training set results from the mismatch in data distribution between the two sets, not from overfitting.**\n",
        "\n",
        "6. **The models exhibited their highest performance on the test data when evaluated using metrics specific to class 1.**\n",
        "\n",
        "    ### 1. Precision: Naive Bayes - 0.2773.\n",
        "    ### 2. Recall: SVM - 0.7124.\n",
        "    ### 3. Accuracy: Naive Bayes - 0.7532.\n",
        "    ### 4. ROC-AUC: SVM - 0.6745.\n",
        "    ### 5. F1 macro: SVM - 0.3785.\n",
        "\n",
        "7. **The SVM model, was chosen as the ultimate predictive model due to its impressive recall score.**\n",
        "\n",
        "8. **Innovative techniques such as SMOTE combined with Tomek links undersampling and MinMax scalar scaling were implemented to address imbalanced data, resulting in improved model accuracy.**\n",
        "\n",
        "9. **This project serves as a noteworthy illustration of how machine learning techniques can be applied to real-world challenges, delivering positive outcomes for businesses.**\n",
        "\n",
        "Overall, this study underscores the significance of meticulous data preparation and analysis in machine learning initiatives. By dedicating effort to cleaning and transforming the data, selecting pertinent features, and opting for suitable models, precise predictions can be achieved, facilitating informed decision-making across various domains.\n",
        "\n"
      ],
      "metadata": {
        "id": "SuBtlgS_3flw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Thankyou***"
      ],
      "metadata": {
        "id": "ZVmpj0-njPnI"
      }
    }
  ]
}